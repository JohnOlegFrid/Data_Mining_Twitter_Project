{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pylab inline\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "import heapq\n",
    "import operator\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(words_string):    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_free=\" \".join([i for i in words_string.lower().split() if i not in stop])\n",
    "    punc_free=''.join(ch if ch not in exclude else ' ' for ch in stop_free)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "def uniteLists(list_of_lists_of_words):\n",
    "    united=[]\n",
    "    for list_of_words in list_of_lists_of_words:\n",
    "        united+=list_of_words\n",
    "    return united\n",
    "def devideToTermsAndFreq(tfList):\n",
    "    ans_terms=[]\n",
    "    ans_freq=[]\n",
    "    for key,val in tfList:\n",
    "        ans_terms.append(key)\n",
    "        ans_freq.append(val)\n",
    "    return ans_terms,ans_freq\n",
    "def createPlot(x_data,y_data,y_name,x_name,title,i,j,k,color,bartype):\n",
    "    axis_font = {'family': 'serif','color':  'black','weight': 'normal','size': 18,}\n",
    "    title_font = {'family': 'serif','color':  'black','weight': 'normal','size': 22,}\n",
    "    font_size=16\n",
    "    df=pd.DataFrame(data={x_name:x_data,y_name:y_data})\n",
    "    temp = df.pivot_table(values=y_name,index=x_data)\n",
    "    temp.sort_values(by=(y_name), ascending=True,inplace=True)\n",
    "    ax = fig.add_subplot(i,j,k) #121\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(font_size)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(font_size)\n",
    "    ax.set_xlabel(x_name,fontdict=axis_font)\n",
    "    ax.set_ylabel(y_name,fontdict=axis_font)\n",
    "    ax.set_title(title,fontdict=title_font)\n",
    "    temp.plot(kind=bartype,ax=ax,cmap=color)\n",
    "    \n",
    "def findColDiff(df,col_name1,col_name2):\n",
    "    return df[[col_name1,col_name2]][data_df[col_name1] != data_df[col_name2]]\n",
    "\n",
    "def printNewCols(start,end):\n",
    "    for i in range(start,end) :\n",
    "        print(data_df.iloc[i]['text'])\n",
    "        print(data_df.iloc[i]['clean_text'])\n",
    "        print(data_df.iloc[i]['clean_words_list'])\n",
    "        print(data_df.iloc[i]['hashtags'])\n",
    "        print(data_df.iloc[i]['user_tag'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_df=pd.read_csv('gender-classifier-DFE-791531.csv',header=0,encoding = 'latin1') #iso-8859-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular expressions for text filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"(?:[:=;][oO\\-]?[D\\)\\]\\(\\]/\\\\OpP])\"\"\"\n",
    "html_str=r'<[^>]+>'\n",
    "user_tags_str=r'(?:@[\\w_]+)'\n",
    "hashtags_str='#[a-zA-Z0-9]+'\n",
    "url_str= r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
    "number_str= r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)'\n",
    "another_str=r\"(?:[a-zA-Z0-9#@][a-zA-Z0-9#@'\\-_]+[a-zA-Z0-9#@])\" # words with - and '\n",
    "letters_and_tags_str='[a-z\\'A-Z0-9#@][a-z\\'A-Z0-9#@.-_]*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning the data\n",
    "- **'text'**- sentence per user cleaned from http links and garbage letters.\n",
    "- **'hashtag'**- list of #hashtags per user.\n",
    "- **'user_tag'** - list of @user_tags per user.\n",
    "- **'clean_text'**- same as 'text' just without #tags and @tags.\n",
    "- **'clean_words_list'**- same as 'clean_text' just splitted to list of words per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[data_df['gender'].apply(lambda x : (x=='male' or x=='female') )]\n",
    "data_df['emo']= data_df['text'].apply(lambda x : re.findall(emoticons_str,x))\n",
    "\n",
    "data_df['text']=data_df['text'].apply(lambda x :  re.sub(url_str,\"\",x))\n",
    "data_df['text']=data_df['text'].apply(lambda x :  \" \".join(re.findall(r'('+'|'.join([letters_and_tags_str,emoticons_str])+')',x)))\n",
    "data_df['text']=data_df['text'].apply(lambda x :  re.sub('\\'','',x))\n",
    "data_df['text']=data_df['text'].apply(lambda x :  re.sub('-','',x))\n",
    "data_df['text']=data_df['text'].apply(lambda x :  re.sub('_','',x))\n",
    "data_df['text']=data_df['text'].apply(lambda x :  re.sub('[\\s]+',' ',x))\n",
    "\n",
    "data_df['hashtags']=data_df['text'].apply(lambda x :  re.findall(hashtags_str,x))\n",
    "data_df['user_tag']=data_df['text'].apply(lambda x :  re.findall(user_tags_str,x))\n",
    "data_df['clean_text']=data_df['text'].apply(lambda x :  re.sub('[#@]\\S+\\s*',\"\",x))\n",
    "data_df['clean_text']=data_df['clean_text'].apply(clean)\n",
    "data_df['clean_words_list']=data_df['clean_text'].apply(lambda x:re.findall('[a-zA-Z0-9]+',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male/female_tf contains all the terms with there num of appearnces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words=uniteLists(data_df['clean_words_list'][data_df['gender'].apply(lambda x: x=='male')])\n",
    "female_words=uniteLists(data_df['clean_words_list'][data_df['gender'].apply(lambda x: x=='female')])\n",
    "male_tf = Counter(male_words).most_common()\n",
    "female_tf=Counter(female_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot of the num most common terms for male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=20\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "terms,freq=devideToTermsAndFreq(male_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','male tf %s most common'%(num) ,2,1,1,'Accent','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','female tf %s most common'%(num),2,1,2,'Vega10_r','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male/female_hashtag_tf contains all the hashtags with there num of appearnces. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hashtag_words=uniteLists(data_df['hashtags'][data_df['gender'].apply(lambda x: x=='male')])\n",
    "female_hashtag_words=uniteLists(data_df['hashtags'][data_df['gender'].apply(lambda x: x=='female')])\n",
    "male_hashtag_tf = Counter(male_hashtag_words).most_common()\n",
    "female_hashtag_tf=Counter(female_hashtag_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot for num most common #hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=20\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "terms,freq=devideToTermsAndFreq(male_hashtag_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','male tf %s most common #hashtags'%(num),2,1,1,'tab20c','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_hashtag_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','female tf %s most common #hashtags'%(num),2,1,2,'tab20b_r','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male/female_emo_tf contains all the emojies with there num of appearnces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_emo_words=uniteLists(data_df['emo'][data_df['gender'].apply(lambda x: x=='male')])\n",
    "female_emo_words=uniteLists(data_df['emo'][data_df['gender'].apply(lambda x: x=='female')])\n",
    "male_emo_tf = Counter(male_emo_words).most_common()\n",
    "female_emo_tf=Counter(female_emo_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=15\n",
    "fig = plt.figure(figsize=(18,16))\n",
    "terms,freq=devideToTermsAndFreq(male_emo_tf)\n",
    "createPlot(terms[:num],freq[:num],'emojies','Number of instances','male tf %s most common emojies'%(num),2,1,1,'Set2','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_emo_tf)\n",
    "createPlot(terms[:num],freq[:num],'emojies','Number of instances','female tf %s most common emojies'%(num),2,1,2,'Pastel1','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 \n",
    "## Train a machine learning model to predict the gender of the tweet author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeGenderToNum(word):\n",
    "    if word=='male':\n",
    "        return 1 \n",
    "    return 0\n",
    "def changeNumToGender(num):\n",
    "    if num==0:\n",
    "        return 'female' \n",
    "    return 'male'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def benchmark(clf,x_train,y_train):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
    "    t0 = time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(Y_test, pred)\n",
    "    print(\"CV accuracy:   %0.3f\" % scores.mean())\n",
    "    print(\"Y_test accuracy:   %0.3f\" % score)\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2),lowercase=False)\n",
    "\n",
    "data_df['num_gender']=data_df['gender'].apply(changeGenderToNum)\n",
    "\n",
    "test_size=10000\n",
    "\n",
    "word_for_voc=[]\n",
    "\n",
    "# for i in data_df['clean_text'].tolist():\n",
    "#     word_for_voc.append(i)\n",
    "\n",
    "# for i in data_df['hashtags'].tolist():\n",
    "#     for j in i:\n",
    "#         word_for_voc.append(j)\n",
    "        \n",
    "# for i in data_df['user_tag'].tolist():\n",
    "#     for j in i:\n",
    "#         word_for_voc.append(j)\n",
    "        \n",
    "for i in data_df['terms']:\n",
    "    for j in i:\n",
    "        word_for_voc.append(j)\n",
    "        \n",
    "        \n",
    "corpus_train=data_df['text'].tolist()[:test_size]\n",
    "\n",
    "corpus_test=data_df['text'].tolist()[test_size:test_size*2]\n",
    "\n",
    "vectorizer.fit(word_for_voc)\n",
    "\n",
    "X_train = vectorizer.transform(corpus_train)\n",
    "Y_train = data_df['num_gender'][:test_size].tolist()\n",
    "\n",
    "X_test = vectorizer.transform(corpus_test)\n",
    "Y_test = data_df['num_gender'][test_size:test_size*2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)\n",
    "len(word_for_voc)\n",
    "vectorizer.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "SVM=SGDClassifier()\n",
    "NB=MultinomialNB()\n",
    "KN=KNeighborsClassifier(15)\n",
    "for clf, name in (\n",
    "        (SVM,\"SVM\"),\n",
    "        (KN, \"kneighbors\"),\n",
    "        (NB,\"Naive Bayes\")\n",
    "    ):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf,X_train,Y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.toarray()\n",
    "Y_train=np.asarray(Y_train)\n",
    "X_test=X_test.toarray()\n",
    "Y_test=np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# split into input (X) and output (Y) variables\n",
    "# create model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[0], input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(X_train.shape[0], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, epochs=3, batch_size=50)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeNumToGender(SVM.predict(vectorizer.transform(['there is a football game this monday']).toarray())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most tweeted country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = data_df['tweet_location'][data_df['tweet_location'].apply(lambda x: type(x) == str)]\n",
    "locations=locations.apply(clean)\n",
    "locations_tf = Counter(locations).most_common()\n",
    "locations_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "consumer_key = 'aZgjP0NZeM4iiBaQXJcyOKLJB'\n",
    "consumer_secret = 'VRLyop5JLP8Kh91CMnE0rpOKUhKEtrrVTCOozHVOd8QEFx69XX'\n",
    "access_token = '326266960-9RZItG8Q4FUER0BOX4eO8mrvsvrrDWzaWiJqoens'\n",
    "access_secret = 'v6UHn9puTUv5OgcDwGqf3UxfCZv6XSJZ1gB8UkXOFrnQo'\n",
    "\n",
    "auth = OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILE_NAME='usa_tweets.json'\n",
    "MAX_NUMBER_OF_TWEETS = 10\n",
    "tweets_gathered = 0\n",
    "f = open(JSON_FILE_NAME, 'w')\n",
    "f.write('[')\n",
    "#writer = csv.writer(f)\n",
    "class MyListener(StreamListener):\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        global tweets_gathered\n",
    "        if MAX_NUMBER_OF_TWEETS > tweets_gathered:\n",
    "            tweets_gathered += 1\n",
    "            try:\n",
    "                #print(api.get_user(json.loads(data)['user']['id'])['gender']) - sadly will not work..\n",
    "                f.write(data.rstrip() + ',')\n",
    "                return True\n",
    "            except BaseException as e:\n",
    "                print(\"Error on_data: %s\" % str(e))\n",
    "            return True\n",
    "        else:\n",
    "            f.seek\n",
    "            return False\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "# those coordinates are boxed that represents most of america - the country that we've found to have the most tweetings. \n",
    "twitter_stream.filter(locations=[-124.89,32.7,-93.07,48.99,\n",
    "                                 -110.64,31.98,-84.43,47.01,\n",
    "                                -99.04,26.29,-72.83,41.58,\n",
    "                                -79.0,32.1,-70.6,43.69,\n",
    "                                -75.3,41.06,-66.91,45.06])\n",
    "f.close()\n",
    "#just a fix to the json file\n",
    "with open(JSON_FILE_NAME, 'rb+') as f:\n",
    "    f.seek(-1, os.SEEK_END)\n",
    "    f.write(b']')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_twitts_df = pd.read_json(path_or_buf=JSON_FILE_NAME)\n",
    "display(new_twitts_df)\n",
    "# Can't get twitts gender - arghhhh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
