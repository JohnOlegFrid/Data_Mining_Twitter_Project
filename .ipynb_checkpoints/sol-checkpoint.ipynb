{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter user gender classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pylab inline\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import matplotlib as plt\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "import heapq\n",
    "import operator\n",
    "import re\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "In this question we've preprocessed the data in our csv file and analyzed the given data.\n",
    "We've printed out plots which indicates what are the most commonly used terms and hashtags used by the two opposite genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from the file\n",
    "data_df=pd.read_csv('gender-classifier-DFE-791531.csv',header=0,encoding = 'latin1') #iso-8859-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the metadata of our training experiment.\n",
    "# We can see what are our columns\n",
    "print(data_df.axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(words_string):\n",
    "    '''\n",
    "    Returns a string after removing stop words and punctuations, lowering all characters and normalizing the words.\n",
    "    '''\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_free=\" \".join([i for i in words_string.lower().split() if i not in stop])\n",
    "    punc_free=''.join(ch if ch not in exclude else ' ' for ch in stop_free)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "def uniteLists(list_of_lists_of_words):\n",
    "    '''\n",
    "    Doing flat reduce, returning only one list of strings \n",
    "    '''\n",
    "    united=[]\n",
    "    for list_of_words in list_of_lists_of_words:\n",
    "        united+=list_of_words\n",
    "    return united\n",
    "def devideToTermsAndFreq(tfList):\n",
    "    '''\n",
    "    Returns all the terms and their frequencies in the given list.\n",
    "    '''\n",
    "    ans_terms=[]\n",
    "    ans_freq=[]\n",
    "    for key,val in tfList:\n",
    "        ans_terms.append(key)\n",
    "        ans_freq.append(val)\n",
    "    return ans_terms,ans_freq\n",
    "def createPlot(x_data,y_data,y_name,x_name,title,i,j,k,color,bartype):\n",
    "    '''\n",
    "    Prints a plot for the given inputs:\n",
    "    x_data - an array for the x axes.\n",
    "    y_data - an array for the y axes.\n",
    "    y_name - the label for the y axes.\n",
    "    x_name - the label for the x axes.\n",
    "    title - title for the plot.\n",
    "    i,j,k,color,bartype - some data for the decoration of the graph.\n",
    "    '''\n",
    "    axis_font = {'family': 'serif','color':  'black','weight': 'normal','size': 18,}\n",
    "    title_font = {'family': 'serif','color':  'black','weight': 'normal','size': 22,}\n",
    "    font_size=16\n",
    "    df=pd.DataFrame(data={x_name:x_data,y_name:y_data})\n",
    "    temp = df.pivot_table(values=y_name,index=x_data)\n",
    "    temp.sort_values(by=(y_name), ascending=True,inplace=True)\n",
    "    ax = fig.add_subplot(i,j,k) #121\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(font_size)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(font_size)\n",
    "    ax.set_xlabel(x_name,fontdict=axis_font)\n",
    "    ax.set_ylabel(y_name,fontdict=axis_font)\n",
    "    ax.set_title(title,fontdict=title_font)\n",
    "    temp.plot(kind=bartype,ax=ax,cmap=color)\n",
    "\n",
    "def findColDiff(df,col_name1,col_name2):\n",
    "    return df[[col_name1,col_name2]][data_df[col_name1] != data_df[col_name2]]\n",
    "\n",
    "def printNewCols(start,end):\n",
    "    for i in range(start,end) :\n",
    "        print(data_df.iloc[i]['text'])\n",
    "        print(data_df.iloc[i]['clean_text'])\n",
    "        print(data_df.iloc[i]['clean_words_list'])\n",
    "        print(data_df.iloc[i]['hashtags'])\n",
    "        print(data_df.iloc[i]['user_tag'])\n",
    "        print()\n",
    "def printCell(df,rowNum,colName)        \n",
    "    print(data_df.loc[rowNum][colName])\n",
    "    print(data_df.loc[rowNum][colName])\n",
    "    print(data_df.loc[rowNum][colName])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular expressions for text filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"(?:[:=;][oO\\-]?[D\\)\\]\\(\\]/\\\\OpP])\"\"\"\n",
    "html_str=r'<[^>]+>'\n",
    "user_tags_str=r'(?:@[\\w_]+)'\n",
    "hashtags_str='#[a-zA-Z0-9]+'\n",
    "url_str= r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
    "number_str= r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)'\n",
    "another_str=r\"(?:[a-zA-Z0-9#@][a-zA-Z0-9#@'\\-_]+[a-zA-Z0-9#@])\" # words with - and '\n",
    "letters_and_tags_str='[a-z\\'A-Z0-9#@][a-z\\'A-Z0-9#@.-_]*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **'text'**- sentence per user cleaned from http links and garbage letters.\n",
    "- **'hashtag'**- list of #hashtags per user.\n",
    "- **'user_tag'** - list of @user_tags per user.\n",
    "- **'clean_text'**- same as 'text' just without #tags and @tags.\n",
    "- **'clean_words_list'**- same as 'clean_text' just splitted to list of words per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCleanedDataFromTweets(data_df):\n",
    "    data_df = data_df[data_df['gender'].apply(lambda x : (x=='male' or x=='female') )]\n",
    "    data_df['emo']= data_df['text'].apply(lambda x : re.findall(emoticons_str,x))\n",
    "\n",
    "    data_df['text']=data_df['text'].apply(lambda x :  re.sub(url_str,\"\",x))\n",
    "    data_df['text']=data_df['text'].apply(lambda x :  \" \".join(re.findall(r'('+'|'.join([letters_and_tags_str,emoticons_str])+')',x)))\n",
    "    data_df['text']=data_df['text'].apply(lambda x :  re.sub('\\'','',x))\n",
    "    data_df['text']=data_df['text'].apply(lambda x :  re.sub('-','',x))\n",
    "    data_df['text']=data_df['text'].apply(lambda x :  re.sub('_','',x))\n",
    "    data_df['text']=data_df['text'].apply(lambda x :  re.sub('[\\s]+',' ',x))\n",
    "\n",
    "    data_df['hashtags']=data_df['text'].apply(lambda x :  re.findall(hashtags_str,x))\n",
    "    data_df['user_tag']=data_df['text'].apply(lambda x :  re.findall(user_tags_str,x))\n",
    "    data_df['clean_text']=data_df['text'].apply(lambda x :  re.sub('[#@]\\S+\\s*',\"\",x))\n",
    "    data_df['clean_text']=data_df['clean_text'].apply(clean)\n",
    "    data_df['clean_words_list']=data_df['clean_text'].apply(lambda x:re.findall('[a-zA-Z0-9]+',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male/female_tf contains all the terms with their frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfByGender(data_df, gender):\n",
    "    gender_words=uniteLists(data_df['clean_words_list'][data_df['gender'].apply(lambda x: x==gender)])\n",
    "    return Counter(gender_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot of the number of most common terms for male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment\n",
    "data_df = extractCleanedDataFromTweets(data_df)\n",
    "male_tf = getTfByGender(data_df, 'male')\n",
    "female_tf = getTfByGender(data_df, 'female')\n",
    "\n",
    "num=15\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "terms,freq=devideToTermsAndFreq(male_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','male tf %s most common'%(num) ,2,1,1,'Accent','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','female tf %s most common'%(num),2,1,2,'Vega10_r','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male/female_hashtag_tf contains all the hashtags with their frequencies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashtagsByGender(data_df, gender):\n",
    "    hashtag_words=uniteLists(data_df['hashtags'][data_df['gender'].apply(lambda x: x==gender)])\n",
    "    return Counter(hashtag_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot for num most common #hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hashtag_tf = getHashtagsByGender(data_df, 'male')\n",
    "female_hashtag_tf = getHashtagsByGender(data_df, 'female')\n",
    "\n",
    "num=15\n",
    "fig = plt.figure(figsize=(18,16))\n",
    "terms,freq=devideToTermsAndFreq(male_hashtag_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','male tf %s most common #hashtags'%(num),2,1,1,'Vega20c','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_hashtag_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','female tf %s most common #hashtags'%(num),2,1,2,'Vega20b_r','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male/female_emo_tf contains all the emojies with there num of appearnces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_emo_words=uniteLists(data_df['emo'][data_df['gender'].apply(lambda x: x=='male')])\n",
    "female_emo_words=uniteLists(data_df['emo'][data_df['gender'].apply(lambda x: x=='female')])\n",
    "male_emo_tf = Counter(male_emo_words).most_common()\n",
    "female_emo_tf=Counter(female_emo_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=15\n",
    "fig = plt.figure(figsize=(18,16))\n",
    "terms,freq=devideToTermsAndFreq(male_emo_tf)\n",
    "createPlot(terms[:num],freq[:num],'emojies','Number of instances','male tf %s most common emojies'%(num),2,1,1,'Set2','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_emo_tf)\n",
    "createPlot(terms[:num],freq[:num],'emojies','Number of instances','female tf %s most common emojies'%(num),2,1,2,'Pastel1','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "# Train a machine learning model to predict the gender of the tweet author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### change gender col in df from string to numeric and vice versa\n",
    "def changeGenderToNum(word): \n",
    "    if word=='male':\n",
    "        return 1 \n",
    "    return 0\n",
    "def changeNumToGender(num):\n",
    "    if num==0:\n",
    "        return 'female' \n",
    "    return 'male'\n",
    "####################################\n",
    "\n",
    "#################################### help us to run multiple classifiction algorithms easier.\n",
    "def benchmark(clf,x_train,y_train): \n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
    "    t0 = time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(Y_test, pred)\n",
    "    print(\"CV accuracy:   %0.3f\" % scores.mean())\n",
    "    print(\"Y_test accuracy:   %0.3f\" % score)\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2),lowercase=False)\n",
    "\n",
    "data_df['num_gender']=data_df['gender'].apply(changeGenderToNum)\n",
    "\n",
    "test_size=10000\n",
    "\n",
    "word_for_voc=[]\n",
    "\n",
    "# for i in data_df['clean_text'].tolist():\n",
    "#     word_for_voc.append(i)\n",
    "\n",
    "# for i in data_df['hashtags'].tolist():\n",
    "#     for j in i:\n",
    "#         word_for_voc.append(j)\n",
    "        \n",
    "# for i in data_df['user_tag'].tolist():\n",
    "#     for j in i:\n",
    "#         word_for_voc.append(j)\n",
    "        \n",
    "for i in data_df['terms']:\n",
    "    for j in i:\n",
    "        word_for_voc.append(j)\n",
    "        \n",
    "        \n",
    "corpus_train=data_df['text'].tolist()[:test_size]\n",
    "\n",
    "corpus_test=data_df['text'].tolist()[test_size:test_size*2]\n",
    "\n",
    "vectorizer.fit(word_for_voc)\n",
    "\n",
    "X_train = vectorizer.transform(corpus_train)\n",
    "Y_train = data_df['num_gender'][:test_size].tolist()\n",
    "\n",
    "X_test = vectorizer.transform(corpus_test)\n",
    "Y_test = data_df['num_gender'][test_size:test_size*2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)\n",
    "len(word_for_voc)\n",
    "vectorizer.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results = []\n",
    "SVM=SGDClassifier()\n",
    "NB=MultinomialNB()\n",
    "KN=KNeighborsClassifier(15)\n",
    "for clf, name in (\n",
    "        (SVM,\"SVM\"),\n",
    "        (KN, \"kneighbors\"),\n",
    "        (NB,\"Naive Bayes\")\n",
    "    ):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf,X_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.toarray()\n",
    "Y_train=np.asarray(Y_train)\n",
    "X_test=X_test.toarray()\n",
    "Y_test=np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# split into input (X) and output (Y) variables\n",
    "# create model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[0], input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(X_train.shape[0], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, epochs=3, batch_size=50)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most tweeted country\n",
    "We've tried to find which country has the biggest number of tweets in our datasets,\n",
    "as we can see, combining all the tweets from all the states of USA reveals that it is the most tweeted country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = data_df['tweet_location'][data_df['tweet_location'].apply(lambda x: type(x) == str)]\n",
    "locations=locations.apply(clean)\n",
    "locations_tf = Counter(locations).most_common()\n",
    "locations_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and functionality initialization for our interaction with twitter api\n",
    "\n",
    "consumer_key = 'aZgjP0NZeM4iiBaQXJcyOKLJB'\n",
    "consumer_secret = 'VRLyop5JLP8Kh91CMnE0rpOKUhKEtrrVTCOozHVOd8QEFx69XX'\n",
    "access_token = '326266960-9RZItG8Q4FUER0BOX4eO8mrvsvrrDWzaWiJqoens'\n",
    "access_secret = 'v6UHn9puTUv5OgcDwGqf3UxfCZv6XSJZ1gB8UkXOFrnQo'\n",
    "\n",
    "auth = OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_MALE_FILE_NAME='male_tweets.json'\n",
    "JSON_FEMALE_FILE_NAME='female_tweets.json'\n",
    "MAX_NUMBER_OF_TWEETS = 219\n",
    "MALE_FILTER_HASHTAG = '#nba'\n",
    "FEMALE_FILTER_HASHTAG = '#femalefilmmakerfriday'\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    def __init__(self, jsonFileName):\n",
    "        self.jsonFileName = jsonFileName\n",
    "        self.f = open(jsonFileName, 'w')\n",
    "        self.f.write('[')\n",
    "        self.tweets_gathered = 0\n",
    "    \n",
    "    def on_data(self, data):\n",
    "    #def on_status(self, status):\n",
    "        #global tweets_gathered\n",
    "        if MAX_NUMBER_OF_TWEETS > self.tweets_gathered:\n",
    "            self.tweets_gathered += 1\n",
    "            try:\n",
    "                print(self.tweets_gathered)\n",
    "                self.f.write(data.rstrip() + ',')\n",
    "                return True\n",
    "            except BaseException as e:\n",
    "                print(\"Error on_data: %s\" % str(e))\n",
    "            return True\n",
    "        else:\n",
    "            self.f.seek\n",
    "            return False\n",
    "    def on_error(self, status):\n",
    "        print(\"Error\" + status)\n",
    "        return True\n",
    "    def __del__(self):\n",
    "        self.f.close()\n",
    "        #just a fix to the json file\n",
    "        with open(self.jsonFileName, 'rb+') as self.f:\n",
    "            self.f.seek(-1, os.SEEK_END)\n",
    "            self.f.write(b']')    \n",
    "        print('died')\n",
    "    def on_exception(self, exception):\n",
    "        print(exception)\n",
    "\n",
    "def generateTweets(isMalesGender):\n",
    "    if isMalesGender:\n",
    "        jsonFileName = JSON_MALE_FILE_NAME\n",
    "        filterHashtags = MALE_FILTER_HASHTAG\n",
    "    else:\n",
    "        jsonFileName = JSON_FEMALE_FILE_NAME\n",
    "        filterHashtags = FEMALE_FILTER_HASHTAG\n",
    "    myList = MyListener(jsonFileName)\n",
    "    twitter_stream = Stream(auth, myList)\n",
    "    # I think Asi said she gives up on the location part because students told her it is somewhat problematic with the other fiter.\n",
    "    # those coordinates are boxed that represents most of america - the country that we've found to have the most tweetings. \n",
    "    #twitter_stream.filter(locations=[-124.89,32.7,-93.07,48.99,\n",
    "    #                                 -110.64,31.98,-84.43,47.01,\n",
    "    #                                -99.04,26.29,-72.83,41.58,\n",
    "    #                                -79.0,32.1,-70.6,43.69,\n",
    "    #                                -75.3,41.06,-66.91,45.06])\n",
    "    twitter_stream.filter(track=[filterHashtags])\n",
    "     \n",
    "# From here we generate the two json files, one for males a\n",
    "#generateTweets(True)\n",
    "#generateTweets(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_twitts_male_df = pd.read_json(path_or_buf=JSON_MALE_FILE_NAME)\n",
    "new_twitts_female_df = pd.read_json(path_or_buf=JSON_FEMALE_FILE_NAME)\n",
    "new_twitts_male_df['gender'] = 'male'\n",
    "new_twitts_female_df['gender'] = 'female'\n",
    "\n",
    "new_twitts_df = new_twitts_male_df.append(new_twitts_female_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_twitts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes all the places part of the tweet was missing\n",
    "for i in range(0, new_twitts_df.shape[0]):\n",
    "    if pd.notnull(new_twitts_df.loc[i,'extended_tweet']):\n",
    "        new_twitts_df['text'][i] = new_twitts_df['extended_tweet'][i]['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePairFromList(lst, el):\n",
    "    for pair in lst:\n",
    "        if pair[0] == el:\n",
    "            lst.remove(pair)\n",
    "\n",
    "new_twitts_df = extractCleanedDataFromTweets(new_twitts_df)\n",
    "new_twitts_male_tf = getTfByGender(new_twitts_df, 'male')\n",
    "new_twitts_female_tf = getTfByGender(new_twitts_df, 'female')\n",
    "removePairFromList(new_twitts_male_tf, 'rt')\n",
    "removePairFromList(new_twitts_female_tf, 'rt')\n",
    "\n",
    "num=15\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "terms,freq=devideToTermsAndFreq(new_twitts_male_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','male tf %s most common'%(num) ,2,1,1,'Accent','barh')\n",
    "terms,freq=devideToTermsAndFreq(new_twitts_female_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','female tf %s most common'%(num),2,1,2,'Vega10_r','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot for num most common #hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "male_hashtag_tf = getHashtagsByGender(new_twitts_df, 'male')\n",
    "female_hashtag_tf = getHashtagsByGender(new_twitts_df, 'female')\n",
    "removePairFromList(male_hashtag_tf, '#NBA')\n",
    "removePairFromList(male_hashtag_tf, '#nba')\n",
    "removePairFromList(female_hashtag_tf, '#FemaleFilmmakerFriday')\n",
    "removePairFromList(female_hashtag_tf, '#femalefilmmakerfriday')\n",
    "\n",
    "num=15\n",
    "fig = plt.figure(figsize=(18,16))\n",
    "terms,freq=devideToTermsAndFreq(male_hashtag_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','male tf %s most common #hashtags'%(num),2,1,1,'Vega20c','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_hashtag_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','female tf %s most common #hashtags'%(num),2,1,2,'Vega20b_r','barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
