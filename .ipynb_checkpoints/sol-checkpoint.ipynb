{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[nltk_data] Downloading package punkt to /home/oleg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/oleg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/oleg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['time']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% pylab inline\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "import heapq\n",
    "import operator\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_df=pd.read_csv('gender-classifier-DFE-791531.csv',header=0,encoding = 'latin1') #iso-8859-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(words_string):    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_free=\" \".join([i for i in words_string.lower().split() if i not in stop])\n",
    "    punc_free=''.join(ch if ch not in exclude else ' ' for ch in stop_free)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "def uniteLists(list_of_lists_of_words):\n",
    "    united=[]\n",
    "    for list_of_words in list_of_lists_of_words:\n",
    "        united+=list_of_words\n",
    "    return united\n",
    "def devideToTermsAndFreq(tfList):\n",
    "    ans_terms=[]\n",
    "    ans_freq=[]\n",
    "    for key,val in tfList:\n",
    "        ans_terms.append(key)\n",
    "        ans_freq.append(val)\n",
    "    return ans_terms,ans_freq\n",
    "def createPlot(x_data,y_data,y_name,x_name,title,i,j,k,color,bartype):\n",
    "    axis_font = {'family': 'serif','color':  'black','weight': 'normal','size': 18,}\n",
    "    title_font = {'family': 'serif','color':  'black','weight': 'normal','size': 22,}\n",
    "    font_size=16\n",
    "    df=pd.DataFrame(data={x_name:x_data,y_name:y_data})\n",
    "    temp = df.pivot_table(values=y_name,index=x_data)\n",
    "    temp.sort_values(by=(y_name), ascending=True,inplace=True)\n",
    "    ax = fig.add_subplot(i,j,k) #121\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(font_size)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(font_size)\n",
    "    ax.set_xlabel(x_name,fontdict=axis_font)\n",
    "    ax.set_ylabel(y_name,fontdict=axis_font)\n",
    "    ax.set_title(title,fontdict=title_font)\n",
    "    temp.plot(kind=bartype,ax=ax,cmap=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data_df = data_df[data_df['gender'].apply(lambda x : (x=='male' or x=='female') )]\n",
    "data_df['text']=data_df['text'].apply(lambda x :  re.sub('(htt\\S+)',\"\",x))\n",
    "data_df['text']=data_df['text'].apply(lambda x :  re.sub('[^a-zA-Z0-9 #@]+',\"\",x))\n",
    "data_df['hashtags']=data_df['text'].apply(lambda x :  re.findall('#[a-zA-Z]+',x))\n",
    "data_df['user_tag']=data_df['text'].apply(lambda x :  re.findall('@[a-zA-Z]+',x))\n",
    "data_df['clean_text']=data_df['text'].apply(lambda x :  re.sub('[#@]\\S+\\s*',\"\",x))\n",
    "data_df['clean_text']=data_df['clean_text'].apply(clean)\n",
    "data_df['clean_words_list']=data_df['clean_text'].apply(lambda x:re.findall('[a-zA-Z0-9]+',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **'text'**- sentence per user cleaned from http links and garbage letters.\n",
    "- **'hashtag'**- list of #hashtags per user.\n",
    "- **'user_tag'** - list of @user_tags per user.\n",
    "- **'clean_text'**- same as 'text' just without #tags and @tags.\n",
    "- **'clean_words_list'**- same as 'clean_text' just splitted to list of words per user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male/female_tf contains all the terms with there frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words=uniteLists(data_df['clean_words_list'][data_df['gender'].apply(lambda x: x=='male')])\n",
    "female_words=uniteLists(data_df['clean_words_list'][data_df['gender'].apply(lambda x: x=='female')])\n",
    "male_tf = Counter(male_words).most_common()\n",
    "female_tf=Counter(female_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot of the num most common terms for male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=15\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "terms,freq=devideToTermsAndFreq(male_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','male tf %s most common'%(num) ,2,1,1,'Accent','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','female tf %s most common'%(num),2,1,2,'Vega10_r','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### male/female_hashtag_tf contains all the hashtags with there frequencies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_hashtag_words=uniteLists(data_df['hashtags'][data_df['gender'].apply(lambda x: x=='male')])\n",
    "female_hashtag_words=uniteLists(data_df['hashtags'][data_df['gender'].apply(lambda x: x=='female')])\n",
    "male_hashtag_tf = Counter(male_hashtag_words).most_common()\n",
    "female_hashtag_tf=Counter(female_hashtag_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot for num most common #hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=15\n",
    "fig = plt.figure(figsize=(18,16))\n",
    "terms,freq=devideToTermsAndFreq(male_hashtag_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','male tf %s most common #hashtags'%(num),2,1,1,'Vega20c','barh')\n",
    "terms,freq=devideToTermsAndFreq(female_hashtag_tf)\n",
    "createPlot(terms[:num],freq[:num],'Terms','Number of instances','female tf %s most common #hashtags'%(num),2,1,2,'Vega20b_r','barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=data_df['text'].tolist()[:500]\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeGenderToNum(word):\n",
    "    if word=='male':\n",
    "        return 1 \n",
    "    return 0\n",
    "def changeNumToGender(num):\n",
    "    if num==0:\n",
    "        return 'female' \n",
    "    return 'male'\n",
    "data_df['num_gender']=data_df['gender'].apply(changeGenderToNum)\n",
    "Y=data_df['num_gender'][:500].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SVM\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "train time: 0.002s\n",
      "test time:  0.000s\n",
      "accuracy:   0.986\n",
      "\n",
      "================================================================================\n",
      "kneighbors\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.001s\n",
      "test time:  0.012s\n",
      "accuracy:   0.572\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.001s\n",
      "test time:  0.000s\n",
      "accuracy:   0.974\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def benchmark(clf,X_train,y_train):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    X_test=X_train\n",
    "    y_test=y_train\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# y_binary = to_categorical(Y)\n",
    "\n",
    "#DLmodel = Sequential()\n",
    "#DLmodel.add(Dense(500,input_shape=(2593,), activation='relu'))\n",
    "#DLmodel.add(Dropout(0.5))\n",
    "#DLmodel.add(Dense(256, activation='sigmoid'))\n",
    "#DLmodel.add(Dropout(0.5))\n",
    "# DLmodel.add(Dense(0, activation='softmax'))\n",
    "# DLmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "\n",
    "# DLmodel.fit(X, y=y_binary, batch_size=2593, nb_epoch=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "\n",
    "# DLmodel.add(Embedding(input_dim=X.shape[0],output_dim= 500, input_length=X.shape[1]))\n",
    "# DLmodel.add(Flatten())\n",
    "# DLmodel.add(Dense(200, activation='sigmoid'))\n",
    "# DLmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "# DLmodel.fit(X, y=Y, batch_size=200, nb_epoch=10, verbose=1, validation_split=0.2)\n",
    "# scores = model.evaluate(X, Y)\n",
    "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "SVM=SGDClassifier()\n",
    "PER=Perceptron()\n",
    "NB=MultinomialNB()\n",
    "KN=KNeighborsClassifier(15)\n",
    "for clf, name in (\n",
    "        (SVM,\"SVM\"),\n",
    "        (KN, \"kneighbors\"),\n",
    "        (NB,\"Naive Bayes\")\n",
    "    ):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf,X,Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changeNumToGender(SVM.predict(vectorizer.transform(['there is a football game this monday']).toarray())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data, categories):\n",
    "    '''\n",
    "    Returns cleaned and normalized documents as shown in class.\n",
    "    '''\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    cleaned_train_with_category = []\n",
    "    for doc, target in zip(data, categories):\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch if ch not in exclude else ' ' for ch in stop_free)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        cleaned_train_with_category.append((normalized.split(),target))\n",
    "    return cleaned_train_with_category\n",
    "\n",
    "def count_terms_in_categories(cleaned_train_with_category, categories):\n",
    "    '''\n",
    "    for every category, counts the number of times each term has been used,\n",
    "    uses a dictionary of category-dictionaries\n",
    "    '''\n",
    "    # an array with the size of the number of dictionaries, \n",
    "    # each dictionary holds a dictionary that maps every term to the number of times it is used\n",
    "    categories_dictionaries = {}\n",
    "    # initialize all categories\n",
    "    for category in categories:\n",
    "        categories_dictionaries[category] = {}\n",
    "    for (terms, category) in cleaned_train_with_category:\n",
    "        for term in terms:\n",
    "            if term in categories_dictionaries[category]:\n",
    "                categories_dictionaries[category][term] = categories_dictionaries[category][term] + 1\n",
    "            else:\n",
    "                categories_dictionaries[category][term] = 1\n",
    "    return categories_dictionaries\n",
    "\n",
    "def order_asc(categories_dictionaries):\n",
    "    '''\n",
    "    Using heaps, stores for each category its 10 most used terms\n",
    "    '''\n",
    "    top_10_terms = {}\n",
    "    for category, terms in categories_dictionaries.iteritems():\n",
    "        heap = [(-value, key) for key,value in terms.items()]\n",
    "        largest = heapq.nsmallest(10, heap)\n",
    "        largest = [key for value, key in largest] #[(key, -value) for value, key in largest]\n",
    "        top_10_terms[category] = largest\n",
    "    return top_10_terms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
