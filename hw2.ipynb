{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some imports to be used in this assignment(\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import heapq\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from math import sqrt\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.extmath import density\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loads a train and test out of the ohsumed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = sklearn.datasets.load_files(container_path=\".\\\\ohsumed-first-20000-docs\\\\training\", categories=None,\n",
    "                                shuffle=True, random_state=42)\n",
    "data_test = sklearn.datasets.load_files(container_path=\".\\\\ohsumed-first-20000-docs\\\\test\", categories=None,\n",
    "                                shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The funtion in this section counts the number of docs from each category. The output of this section is in 'categories count.txt' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C12 Urologic and Male Genital Diseases': 491, 'C21 Disorders of Environmental Origin': 546, 'C22 Animal Diseases': 92, 'C14 Cardiovascular Diseases': 1249, 'C03 Parasitic Diseases': 65, 'C16 Neonatal Diseases and Abnormalities': 200, 'C17 Skin and Connective Tissue Diseases': 295, 'C15 Hemic and Lymphatic Diseases': 215, 'C13 Female Genital Diseases and Pregnancy Complications': 281, 'C23 Pathological Conditions, Signs and Symptoms': 1799, 'C06 Digestive System Diseases': 588, 'C11 Eye Diseases': 162, 'C08 Respiratory Tract Diseases': 473, 'C18 Nutritional and Metabolic Diseases': 388, 'C19 Endocrine Diseases': 191, 'C20 Immunologic Diseases': 525, 'C01 Bacterial Infections and Mycoses': 423, 'C04 Neoplasms': 1163, 'C05 Musculoskeletal Diseases': 283, 'C07 Stomatognathic Diseases': 100, 'C09 Otorhinolaryngologic Diseases': 125, 'C10 Nervous System Diseases': 621, 'C02 Virus Diseases': 158}\n"
     ]
    }
   ],
   "source": [
    "def countCategoriesUsage(categories, documentsCategoriesArray):\n",
    "    '''\n",
    "    Gets categories array and an array of indices of those categories in categories array.\n",
    "    Returns an array of how many docs there are in each category\n",
    "    '''\n",
    "    categoriesDict = {}\n",
    "    for val in documentsCategoriesArray:\n",
    "        if categories[val] in categoriesDict:\n",
    "            categoriesDict[categories[val]]=categoriesDict[categories[val]]+1\n",
    "        else:\n",
    "            categoriesDict[categories[val]] = 1\n",
    "    return categoriesDict\n",
    "    \n",
    "# prints for each category in the train, the amount of documents under specific category\n",
    "print(countCategoriesUsage(data_train.target_names, data_train.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and normalizing the text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've cleaned and normalize our training corpus and above all filtered some used terms for the next reason: \n",
    "# We understood that for better results for some vectorizers (e.g. counting vectorizer), it would be best if we clean common \n",
    "# used terms in our categories, because they would be less meaningful (tf-idf takes this parameter in account so we believe\n",
    "# that not only it filtering those words would not deliver any benefit, but even can degrade its produced data).\n",
    "# Our initial approach was to try and find manually the terms that are in the top ten of many of the categories.\n",
    "# We were aware of the fact that when words are filtered from the top ten most common terms per categories, new terms\n",
    "# can be retrieved and are needed to be added to our filtered words list, thus, the idea of working in iterations have been\n",
    "# created, we filter words in a loop until enough words have been filtered.\n",
    "# The next terms came from iterations over the code in which we found them to be used in more than 0.25 of \n",
    "# the categories. those are the iterations and the terms we found in each of them:\n",
    "# 1. le group cell treatment patient case study disease p\n",
    "# 2. treatment result cell infection may effect\n",
    "# 3. level year one two clinical control syndrome\n",
    "# 4. human therapy 2 associated normal blood\n",
    "# 5. rate 1\n",
    "# 6. antibody\n",
    "# meaning those are our filtered words: set()#u'patient le p group study case disease treatment result cell infection may effect level year one two clinical control syndrome human therapy 2 associated normal blood rate 1 antibody'.split()\n",
    "\n",
    "# After finding all those words to filter, we've decided we can do better by letting our code do those iterations for us.\n",
    "# We've encapsulated all beneath functions in a single function so we can run them again and again until \n",
    "# all top words are filtered (meaning fix-point was reached). We've also printed the words we've filtered in each iteration.\n",
    "\n",
    "def clean():\n",
    "    '''\n",
    "    Returns cleaned and normalized documents as shown in class.\n",
    "    '''\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    cleaned_train_with_category = []\n",
    "    for doc, target in zip(data_train.data, data_train.target):\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch if ch not in exclude else ' ' for ch in stop_free)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        cleaned_train_with_category.append((normalized.split(),data_train.target_names[target]))\n",
    "    return cleaned_train_with_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every category, counts for each term how many times it has been used. using heap data structure, gets the top ten most common terms in each category.\n",
    "#### Note: be aware that even though some of the terms looks meaningless, we've decided to not filter them out because we believe it contributes to better classifying the document by its document structure (e.g. HTML) even though those terms looks meaningful to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_terms_in_categories(cleaned_train_with_category):\n",
    "    '''\n",
    "    for every category, counts the number of times each term has been used,\n",
    "    uses a dictionary of category-dictionaries\n",
    "    '''\n",
    "    # an array with the size of the number of dictionaries, \n",
    "    # each dictionary holds a dictionary that maps every term to the number of times it is used\n",
    "    categories_dictionaries = {}\n",
    "    # initialize all categories\n",
    "    for category in data_train.target_names:\n",
    "        categories_dictionaries[category] = {}\n",
    "    for (terms, category) in cleaned_train_with_category:\n",
    "        for term in terms:\n",
    "            if term in categories_dictionaries[category]:\n",
    "                categories_dictionaries[category][term] = categories_dictionaries[category][term] + 1\n",
    "            else:\n",
    "                categories_dictionaries[category][term] = 1\n",
    "    return categories_dictionaries\n",
    "\n",
    "def filter_words(categories_dictionaries, filtered_words):\n",
    "    '''\n",
    "    Removes terms from the categories_dictionaries we wish to filter\n",
    "    '''\n",
    "    for category in categories_dictionaries.keys():\n",
    "        for filtered_word in filtered_words:\n",
    "            categories_dictionaries[category].pop(filtered_word, None)\n",
    "            \n",
    "def get_top_ten_of_categories(categories_dictionaries):\n",
    "    '''\n",
    "    Using heaps, stores for each category its 10 most used terms\n",
    "    '''\n",
    "    top_10_terms = {}\n",
    "    for category, terms in categories_dictionaries.iteritems():\n",
    "        heap = [(-value, key) for key,value in terms.items()]\n",
    "        largest = heapq.nsmallest(10, heap)\n",
    "        largest = [key for value, key in largest] #[(key, -value) for value, key in largest]\n",
    "        top_10_terms[category] = largest\n",
    "    return top_10_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next functions are responsible for finding the words we want to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_terms_usage_in_top_ten(top_10_terms):\n",
    "    '''\n",
    "    counts the number of times each word has been seen in the given heaps.\n",
    "    '''\n",
    "    terms_counter = {}\n",
    "    for category, terms in top_10_terms.iteritems():\n",
    "        for term in terms:\n",
    "            if term in terms_counter:\n",
    "                terms_counter[term] = terms_counter[term] + 1\n",
    "            else:\n",
    "                terms_counter[term] = 1\n",
    "    return terms_counter\n",
    "                \n",
    "def get_words_needed_to_be_filtered(terms_counter, min_percent_of_categories):\n",
    "    '''\n",
    "    Returns all terms that are needed to be filtered, \n",
    "    a term is needed to be filtered amount of times iff \n",
    "    (number of times it is in the top ten > min_percent_of_categories * number of categories)\n",
    "    '''\n",
    "    to_filter = set()\n",
    "    for term, counter in terms_counter.iteritems():\n",
    "        if(counter > min_percent_of_categories * len(data_train.target_names)):\n",
    "            to_filter.add(term)\n",
    "    return to_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next section is responsible for running all the above as explained in fixed-point fasioned way, and displays a table of the top ten words used for each category after filtering all unwanted words (as described above). The ouput of this section is in 'filtered and top ten.png' file\n",
    "\n",
    "#### Note: we filtered all words that are shown in more than 10% of the categories top words, in our case it means that if a word is in 3 categories or more, it should be filtered.\n",
    "#### Another note: we took note that not too many words are filtered, around 95 terms are filtered out of more than 20,000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the words to filter in the next iteration: set([u'case', u'cell', u'le', u'group', u'p', u'study', u'disease', u'infection', u'1', u'0', u'2', u'treatment', u'human', u'year', u'patient', u'tumor'])\n",
      "the words to filter in the next iteration: set([u'control', u'associated', u'syndrome', u'cancer', u'level', u'may', u'normal', u'two', u'one', u'3', u'clinical', u'5', u'result', u'serum', u'effect', u'antibody', u'day', u'4'])\n",
      "the words to filter in the next iteration: set([u'virus', u'lesion', u'risk', u'child', u's', u'increased', u'three', u'rate', u'tissue', u'therapy', u'blood', u'6', u'artery', u'carcinoma', u'subject'])\n",
      "the words to filter in the next iteration: set([u'10', u'diagnosis', u'hiv', u'age', u'month', u'7', u'factor', u'8', u'surgery', u'stage', u'treated', u'response', u'mean'])\n",
      "the words to filter in the next iteration: set([u'acute', u'woman', u'significantly', u'chronic', u'antigen', u'ventricular', u'pressure', u'compared', u'infected', u'significant', u'found', u'type', u'bone'])\n",
      "the words to filter in the next iteration: set([u'heart', u'mg', u'pain', u'symptom', u'n'])\n",
      "the words to filter in the next iteration: set([u'high', u'time', u'plasma', u'coronary'])\n",
      "the words to filter in the next iteration: set([u'pulmonary', u'complication', u'change'])\n",
      "the words to filter in the next iteration: set([u'cardiac', u'failure', u'survival', u'also'])\n",
      "the words to filter in the next iteration: set([u'positive', u'rat', u'greater'])\n",
      "the words to filter in the next iteration: set([u'lung'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C01</th>\n",
       "      <th>C02</th>\n",
       "      <th>C03</th>\n",
       "      <th>C04</th>\n",
       "      <th>C05</th>\n",
       "      <th>C06</th>\n",
       "      <th>C07</th>\n",
       "      <th>C08</th>\n",
       "      <th>C09</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>C16</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>immunodeficiency</td>\n",
       "      <td>parasite</td>\n",
       "      <td>breast</td>\n",
       "      <td>knee</td>\n",
       "      <td>gastric</td>\n",
       "      <td>periodontal</td>\n",
       "      <td>respiratory</td>\n",
       "      <td>ear</td>\n",
       "      <td>cerebral</td>\n",
       "      <td>eye</td>\n",
       "      <td>renal</td>\n",
       "      <td>pregnancy</td>\n",
       "      <td>left</td>\n",
       "      <td>anemia</td>\n",
       "      <td>infant</td>\n",
       "      <td>skin</td>\n",
       "      <td>insulin</td>\n",
       "      <td>glucose</td>\n",
       "      <td>aid</td>\n",
       "      <td>injury</td>\n",
       "      <td>model</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>culture</td>\n",
       "      <td>aid</td>\n",
       "      <td>falciparum</td>\n",
       "      <td>primary</td>\n",
       "      <td>hip</td>\n",
       "      <td>liver</td>\n",
       "      <td>gland</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>nasal</td>\n",
       "      <td>brain</td>\n",
       "      <td>retinal</td>\n",
       "      <td>bladder</td>\n",
       "      <td>fetal</td>\n",
       "      <td>myocardial</td>\n",
       "      <td>hemoglobin</td>\n",
       "      <td>defect</td>\n",
       "      <td>arthritis</td>\n",
       "      <td>glucose</td>\n",
       "      <td>insulin</td>\n",
       "      <td>immunodeficiency</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>animal</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bacterial</td>\n",
       "      <td>htlv</td>\n",
       "      <td>malaria</td>\n",
       "      <td>malignant</td>\n",
       "      <td>joint</td>\n",
       "      <td>ulcer</td>\n",
       "      <td>primary</td>\n",
       "      <td>nasal</td>\n",
       "      <td>hearing</td>\n",
       "      <td>muscle</td>\n",
       "      <td>visual</td>\n",
       "      <td>prostate</td>\n",
       "      <td>pregnant</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>platelet</td>\n",
       "      <td>right</td>\n",
       "      <td>rheumatoid</td>\n",
       "      <td>diabetic</td>\n",
       "      <td>thyroid</td>\n",
       "      <td>asthma</td>\n",
       "      <td>fracture</td>\n",
       "      <td>mouse</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antibiotic</td>\n",
       "      <td>i</td>\n",
       "      <td>cerebral</td>\n",
       "      <td>node</td>\n",
       "      <td>arthritis</td>\n",
       "      <td>bowel</td>\n",
       "      <td>oral</td>\n",
       "      <td>increase</td>\n",
       "      <td>loss</td>\n",
       "      <td>stroke</td>\n",
       "      <td>ocular</td>\n",
       "      <td>urinary</td>\n",
       "      <td>maternal</td>\n",
       "      <td>aortic</td>\n",
       "      <td>sickle</td>\n",
       "      <td>congenital</td>\n",
       "      <td>psoriasis</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>diabetic</td>\n",
       "      <td>lymphoma</td>\n",
       "      <td>trauma</td>\n",
       "      <td>induced</td>\n",
       "      <td>graft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h</td>\n",
       "      <td>measles</td>\n",
       "      <td>drug</td>\n",
       "      <td>dna</td>\n",
       "      <td>fracture</td>\n",
       "      <td>bile</td>\n",
       "      <td>dental</td>\n",
       "      <td>infant</td>\n",
       "      <td>laryngeal</td>\n",
       "      <td>nerve</td>\n",
       "      <td>intraocular</td>\n",
       "      <td>dialysis</td>\n",
       "      <td>delivery</td>\n",
       "      <td>flow</td>\n",
       "      <td>transfusion</td>\n",
       "      <td>abnormality</td>\n",
       "      <td>ra</td>\n",
       "      <td>cholesterol</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>test</td>\n",
       "      <td>use</td>\n",
       "      <td>kg</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pylorus</td>\n",
       "      <td>ii</td>\n",
       "      <td>rosetting</td>\n",
       "      <td>metastasis</td>\n",
       "      <td>muscle</td>\n",
       "      <td>biliary</td>\n",
       "      <td>parotid</td>\n",
       "      <td>airway</td>\n",
       "      <td>histamine</td>\n",
       "      <td>showed</td>\n",
       "      <td>macular</td>\n",
       "      <td>function</td>\n",
       "      <td>outcome</td>\n",
       "      <td>ischemia</td>\n",
       "      <td>iron</td>\n",
       "      <td>flow</td>\n",
       "      <td>cutaneous</td>\n",
       "      <td>concentration</td>\n",
       "      <td>hormone</td>\n",
       "      <td>il</td>\n",
       "      <td>ethanol</td>\n",
       "      <td>venous</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sepsis</td>\n",
       "      <td>among</td>\n",
       "      <td>peptide</td>\n",
       "      <td>analysis</td>\n",
       "      <td>showed</td>\n",
       "      <td>esophageal</td>\n",
       "      <td>salivary</td>\n",
       "      <td>small</td>\n",
       "      <td>allergic</td>\n",
       "      <td>stimulation</td>\n",
       "      <td>technique</td>\n",
       "      <td>kidney</td>\n",
       "      <td>among</td>\n",
       "      <td>infarction</td>\n",
       "      <td>marrow</td>\n",
       "      <td>left</td>\n",
       "      <td>contact</td>\n",
       "      <td>dependent</td>\n",
       "      <td>parathyroid</td>\n",
       "      <td>b</td>\n",
       "      <td>used</td>\n",
       "      <td>concentration</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cause</td>\n",
       "      <td>lymphocyte</td>\n",
       "      <td>c</td>\n",
       "      <td>receptor</td>\n",
       "      <td>disc</td>\n",
       "      <td>hepatic</td>\n",
       "      <td>lip</td>\n",
       "      <td>12</td>\n",
       "      <td>rhinitis</td>\n",
       "      <td>seizure</td>\n",
       "      <td>posterior</td>\n",
       "      <td>creatinine</td>\n",
       "      <td>test</td>\n",
       "      <td>death</td>\n",
       "      <td>l</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>inflammatory</td>\n",
       "      <td>niddm</td>\n",
       "      <td>l</td>\n",
       "      <td>beta</td>\n",
       "      <td>burn</td>\n",
       "      <td>dog</td>\n",
       "      <td>postoperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>use</td>\n",
       "      <td>related</td>\n",
       "      <td>leishmaniasis</td>\n",
       "      <td>resection</td>\n",
       "      <td>total</td>\n",
       "      <td>duct</td>\n",
       "      <td>cleft</td>\n",
       "      <td>chest</td>\n",
       "      <td>middle</td>\n",
       "      <td>spinal</td>\n",
       "      <td>detachment</td>\n",
       "      <td>ml</td>\n",
       "      <td>activity</td>\n",
       "      <td>arterial</td>\n",
       "      <td>count</td>\n",
       "      <td>surgical</td>\n",
       "      <td>drug</td>\n",
       "      <td>acid</td>\n",
       "      <td>hypothyroidism</td>\n",
       "      <td>skin</td>\n",
       "      <td>related</td>\n",
       "      <td>observed</td>\n",
       "      <td>surgical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c</td>\n",
       "      <td>seropositive</td>\n",
       "      <td>onchocerciasis</td>\n",
       "      <td>four</td>\n",
       "      <td>activity</td>\n",
       "      <td>stone</td>\n",
       "      <td>report</td>\n",
       "      <td>oxygen</td>\n",
       "      <td>report</td>\n",
       "      <td>finding</td>\n",
       "      <td>glaucoma</td>\n",
       "      <td>prostatic</td>\n",
       "      <td>ectopic</td>\n",
       "      <td>right</td>\n",
       "      <td>increase</td>\n",
       "      <td>cystic</td>\n",
       "      <td>induced</td>\n",
       "      <td>mellitus</td>\n",
       "      <td>hyperparathyroidism</td>\n",
       "      <td>lymphocyte</td>\n",
       "      <td>care</td>\n",
       "      <td>experimental</td>\n",
       "      <td>per</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          C01               C02             C03         C04        C05  \\\n",
       "0           b  immunodeficiency        parasite      breast       knee   \n",
       "1     culture               aid      falciparum     primary        hip   \n",
       "2   bacterial              htlv         malaria   malignant      joint   \n",
       "3  antibiotic                 i        cerebral        node  arthritis   \n",
       "4           h           measles            drug         dna   fracture   \n",
       "5     pylorus                ii       rosetting  metastasis     muscle   \n",
       "6      sepsis             among         peptide    analysis     showed   \n",
       "7       cause        lymphocyte               c    receptor       disc   \n",
       "8         use           related   leishmaniasis   resection      total   \n",
       "9           c      seropositive  onchocerciasis        four   activity   \n",
       "\n",
       "          C06          C07          C08        C09          C10          C11  \\\n",
       "0     gastric  periodontal  respiratory        ear     cerebral          eye   \n",
       "1       liver        gland    pneumonia      nasal        brain      retinal   \n",
       "2       ulcer      primary        nasal    hearing       muscle       visual   \n",
       "3       bowel         oral     increase       loss       stroke       ocular   \n",
       "4        bile       dental       infant  laryngeal        nerve  intraocular   \n",
       "5     biliary      parotid       airway  histamine       showed      macular   \n",
       "6  esophageal     salivary        small   allergic  stimulation    technique   \n",
       "7     hepatic          lip           12   rhinitis      seizure    posterior   \n",
       "8        duct        cleft        chest     middle       spinal   detachment   \n",
       "9       stone       report       oxygen     report      finding     glaucoma   \n",
       "\n",
       "          C12        C13           C14          C15          C16  \\\n",
       "0       renal  pregnancy          left       anemia       infant   \n",
       "1     bladder      fetal    myocardial   hemoglobin       defect   \n",
       "2    prostate   pregnant  hypertension     platelet        right   \n",
       "3     urinary   maternal        aortic       sickle   congenital   \n",
       "4    dialysis   delivery          flow  transfusion  abnormality   \n",
       "5    function    outcome      ischemia         iron         flow   \n",
       "6      kidney      among    infarction       marrow         left   \n",
       "7  creatinine       test         death            l      anomaly   \n",
       "8          ml   activity      arterial        count     surgical   \n",
       "9   prostatic    ectopic         right     increase       cystic   \n",
       "\n",
       "            C17            C18                  C19               C20  \\\n",
       "0          skin        insulin              glucose               aid   \n",
       "1     arthritis        glucose              insulin  immunodeficiency   \n",
       "2    rheumatoid       diabetic              thyroid            asthma   \n",
       "3     psoriasis       diabetes             diabetic          lymphoma   \n",
       "4            ra    cholesterol             diabetes              test   \n",
       "5     cutaneous  concentration              hormone                il   \n",
       "6       contact      dependent          parathyroid                 b   \n",
       "7  inflammatory          niddm                    l              beta   \n",
       "8          drug           acid       hypothyroidism              skin   \n",
       "9       induced       mellitus  hyperparathyroidism        lymphocyte   \n",
       "\n",
       "        C21            C22            C23  \n",
       "0    injury          model             12  \n",
       "1   alcohol         animal           used  \n",
       "2  fracture          mouse           four  \n",
       "3    trauma        induced          graft  \n",
       "4       use             kg             20  \n",
       "5   ethanol         venous              9  \n",
       "6      used  concentration          death  \n",
       "7      burn            dog  postoperative  \n",
       "8   related       observed       surgical  \n",
       "9      care   experimental            per  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the words we wouldbe filtering will be gathered in this variable\n",
    "total_filtered_words = set()\n",
    "def display_top_ten_terms_after_filter(min_percent_of_categories):\n",
    "    '''\n",
    "    Returns a top ten table after filtering all unwanted words.\n",
    "    '''\n",
    "    global total_filtered_words\n",
    "    filtered_words = set(\"just for loop enter\")\n",
    "    top_10_terms = {}\n",
    "    total_filtered_words = set()\n",
    "    cleaned_train_with_category = clean()\n",
    "    categories_dictionaries = count_terms_in_categories(cleaned_train_with_category)\n",
    "    while len(filtered_words) != 0:\n",
    "        top_10_terms = get_top_ten_of_categories(categories_dictionaries)\n",
    "        terms_counter = count_terms_usage_in_top_ten(top_10_terms)\n",
    "        filtered_words = get_words_needed_to_be_filtered(terms_counter, min_percent_of_categories)\n",
    "        total_filtered_words = total_filtered_words.union(filtered_words)\n",
    "        if len(filtered_words) != 0:\n",
    "            print(\"the words to filter in the next iteration: \" + repr(filtered_words))\n",
    "        filter_words(categories_dictionaries, filtered_words)\n",
    "    return top_10_terms\n",
    "   \n",
    "top_10_terms = display_top_ten_terms_after_filter(0.1)\n",
    "display(pd.DataFrame.from_dict(data=top_10_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We believed we could be using the filtered words for acheiving better results when we were using CountVectorizer, and so\n",
    "# we've followed through with the idea we had in mind that we should try this question for both, the original corpus and\n",
    "# a one we've been cleaning and normalizing from question 1.\n",
    "\n",
    "def clean_with_filtered():\n",
    "    '''\n",
    "    get the train_data after fully filtering applied.\n",
    "    '''\n",
    "    global total_filtered_words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    cleaned_train_with_category = []\n",
    "    for doc, target in zip(data_train.data, data_train.target):\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch if ch not in exclude else ' ' for ch in doc.lower())\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        filtered = \" \".join([i for i in normalized.split() if i not in total_filtered_words])\n",
    "        cleaned_train_with_category.append(filtered)\n",
    "    return cleaned_train_with_category\n",
    "\n",
    "cleaned_train_with_category = clean_with_filtered()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We chose to try CountVectorizer and TfidfVectorizer for feature extraction, and Naive Bayes (MultinomialNB), k-nearest neighbors (KNeighborsClassifier) and Neural network (MLPClassifier) for machine learning models. So we've used pipeline for the six pairs:<br>(CountVectorizer, MultinomialNB)<br> (TfidfVectorizer, MultinomialNB)<br>(CountVectorizer, KNeighborsClassifier)<br>(TfidfVectorizer, KNeighborsClassifier)<br>(CountVectorizer, MLPClassifier)<br>(TfidfVectorizer, MLPClassifier)<br>The output to this sections can be found in the attached files by the pair's name, i.e. for the first section: '1 - countVectorizer + naiveBayes.png'\n",
    "#### We were not asked in the assignment document to unify the feature extractors so we've decided it would benefit us most to not use feature union method and not merge the two feature extraction to one, because we truly want to get our hands on those 6 possibilities and make a massive comparison between those 6 (and not just 3).\n",
    "#### We used the next 6 code segments to find out the best parameters for each object in each pair. Initialy, we did all 6 for both, our filtered corpus and the original corpus. Sadly, we found that for some unknown reason, the score of using the original corpus is better than using the filtered corpus. After many tests for this parameteres we've decided to stop to check for both of them because it consumes too much time and so the final output of this sections sometimes not covering the filtered corpus.\n",
    "#### Important note: We wanted  to use k-fold validation to achieve greater results, but you've pointed us that the pipeline already does that for us (truely does - as mentioned in best score description) - which is awesome :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best score: ', 0.31352439375059904)\n",
      "('Best params: ', {'vect__max_df': 0.7, 'vect__min_df': 0.0, 'clf__alpha': 0.8})\n"
     ]
    }
   ],
   "source": [
    "#1 - using CountVectorizer and MultinomialNB\n",
    "nb_clf = Pipeline([('vect', CountVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1))),\n",
    "                   ('clf', MultinomialNB())])\n",
    "\n",
    "parameters = {'clf__alpha' : (0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0),\n",
    "                'vect__min_df':(0.0,0.1,0.2,0.3),\n",
    "                'vect__max_df':(0.7,0.8,0.9,1.0)}\n",
    "\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(data_train.data,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best score: ', 0.26876258027413014)\n",
      "('Best params: ', {'vect__max_df': 0.7, 'vect__min_df': 0.0, 'clf__alpha': 0.1})\n",
      "\n",
      "('Best score: ', 0.2649285919677945)\n",
      "('Best params: ', {'vect__max_df': 0.7, 'vect__min_df': 0.0, 'clf__alpha': 0.1})\n"
     ]
    }
   ],
   "source": [
    "#2 - using TfidfVectorizer and MultinomialNB\n",
    "nb_clf = Pipeline([('vect', TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1))),\n",
    "                   ('clf', MultinomialNB())])\n",
    "\n",
    "parameters = {'clf__alpha' : (0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0),\n",
    "                'vect__min_df':(0.0,0.1,0.2,0.3),\n",
    "                'vect__max_df':(0.7,0.8,0.9,1.0)}\n",
    "\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(data_train.data,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)\n",
    "print\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(cleaned_train_with_category,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best score: ', 0.22026262819898398)\n",
      "('Best params: ', {'clf__weights': 'uniform', 'vect__max_df': 0.7, 'clf__algorithm': 'auto', 'vect__min_df': 0.0, 'clf__n_neighbors': 30})\n",
      "\n",
      "('Best score: ', 0.19045336911722419)\n",
      "('Best params: ', {'clf__weights': 'uniform', 'vect__max_df': 0.7, 'clf__algorithm': 'auto', 'vect__min_df': 0.0, 'clf__n_neighbors': 30})\n"
     ]
    }
   ],
   "source": [
    "#3 - using CountVectorizer and KNeighborsClassifier\n",
    "nb_clf = Pipeline([('vect', CountVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1))),\n",
    "                   ('clf', KNeighborsClassifier())])\n",
    "\n",
    "parameters =  {'clf__n_neighbors':(30,40,50),\n",
    "                'clf__weights':('uniform','distance'), \n",
    "                'clf__algorithm':('auto', 'ball_tree', 'kd_tree', 'brute'),\n",
    "                'vect__min_df':(0.0,0.1,0.2,0.3),\n",
    "                'vect__max_df':(0.7,0.8,0.9,1.0)}\n",
    "\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(data_train.data,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)\n",
    "print\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(cleaned_train_with_category,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best score: ', 0.39739288795169175)\n",
      "('Best params: ', {'clf__weights': 'uniform', 'vect__max_df': 0.7, 'clf__algorithm': 'auto', 'vect__min_df': 0.0, 'clf__n_neighbors': 40})\n",
      "\n",
      "('Best score: ', 0.39183360490750502)\n",
      "('Best params: ', {'clf__weights': 'uniform', 'vect__max_df': 0.7, 'clf__algorithm': 'auto', 'vect__min_df': 0.0, 'clf__n_neighbors': 40})\n"
     ]
    }
   ],
   "source": [
    "#4 - using TfidfVectorizer and KNeighborsClassifier\n",
    "nb_clf = Pipeline([('vect', TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1))),\n",
    "                   ('clf', KNeighborsClassifier())])\n",
    "\n",
    "parameters =  {'clf__n_neighbors':(30,40,50),\n",
    "                'clf__weights':('uniform','distance'), \n",
    "                'clf__algorithm':('auto', 'ball_tree', 'kd_tree', 'brute'),\n",
    "                'vect__min_df':(0.0,0.1,0.2,0.3),\n",
    "                'vect__max_df':(0.7,0.8,0.9,1.0)}\n",
    "\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(data_train.data,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)\n",
    "print\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(cleaned_train_with_category,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best score: ', 0.24125371417617175)\n",
      "('Best params: ', {'clf__hidden_layer_sizes': (102,), 'vect__min_df': 0.0})\n"
     ]
    }
   ],
   "source": [
    "#5 - using CountVectorizer and MLPClassifier\n",
    "nb_clf = Pipeline([('vect', CountVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1), max_df=1.0)),\n",
    "                   ('clf', MLPClassifier())])\n",
    "\n",
    "parameters =  {'vect__min_df':(0.0,0.3),\n",
    "               'clf__hidden_layer_sizes': ((100,),((int(sqrt(len(data_train.data))),)))}\n",
    "\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(data_train.data,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "('Best score: ', 0.24920923991181826)\n",
      "('Best params: ', {'clf__hidden_layer_sizes': (100,), 'vect__min_df': 0.0})\n"
     ]
    }
   ],
   "source": [
    "#6 - using TfidfVectorizer and MLPClassifier\n",
    "nb_clf = Pipeline([('vect', TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1), max_df=1.0)),\n",
    "                   ('clf', MLPClassifier())])\n",
    "\n",
    "parameters =  {'vect__min_df':(0.0,0.3),\n",
    "               'clf__hidden_layer_sizes': ((100,),((int(sqrt(len(data_train.data))),)))}\n",
    "\n",
    "gs_clf = GridSearchCV(nb_clf, parameters, n_jobs=4)\n",
    "gs_clf = gs_clf.fit(data_train.data,data_train.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section we are evaluating each and every vectorizer and model with the best parameters we could've found in the 6 sections above and compare them to each other. You can find the output text in 'final scores.txt' file and the graphs which compare them in 'final scores.png' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Vectorizer: Count vectorizer\n",
      "Model: Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.7, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w...n', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'],\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True)\n",
      "train time: 5.724s\n",
      "test time:  11.627s\n",
      "accuracy:   0.409\n",
      "dimensionality: 30361\n",
      "density: 1.000000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        C01       0.34      0.42      0.38       506\n",
      "        C02       0.41      0.05      0.08       233\n",
      "        C03       1.00      0.04      0.08        70\n",
      "        C04       0.46      0.72      0.56      1467\n",
      "        C05       0.44      0.31      0.36       429\n",
      "        C06       0.45      0.48      0.46       632\n",
      "        C07       0.25      0.01      0.03       146\n",
      "        C08       0.41      0.30      0.35       600\n",
      "        C09       0.39      0.05      0.10       129\n",
      "        C10       0.42      0.45      0.44       941\n",
      "        C11       0.45      0.25      0.32       202\n",
      "        C12       0.51      0.33      0.40       548\n",
      "        C13       0.43      0.31      0.36       386\n",
      "        C14       0.51      0.69      0.58      1301\n",
      "        C15       0.38      0.16      0.23       320\n",
      "        C16       0.28      0.11      0.16       228\n",
      "        C17       0.46      0.30      0.36       348\n",
      "        C18       0.38      0.52      0.44       400\n",
      "        C19       0.42      0.12      0.18       191\n",
      "        C20       0.33      0.51      0.40       695\n",
      "        C21       0.45      0.46      0.45       717\n",
      "        C22       0.33      0.01      0.02        91\n",
      "        C23       0.27      0.25      0.26      2153\n",
      "\n",
      "avg / total       0.40      0.41      0.39     12733\n",
      "\n",
      "confusion matrix:\n",
      "[[ 211    0    0   13    3   29    0   27    0    8    5   18   14   21\n",
      "     1    0    4    5    1   48   16    0   82]\n",
      " [  28   11    0   18    0   10    0   11    0    6    1    0    6    0\n",
      "     0    1    1    3    0  118    3    0   16]\n",
      " [   7    0    3    6    0    4    0    1    0    5    1    0    2    2\n",
      "     1    1    0    3    0   19    1    0   14]\n",
      " [   5    3    0 1049    3   40    1   14    1   45    4   30   15   17\n",
      "    12    4    6   19    5   60   27    0  107]\n",
      " [  24    0    0   38  132    5    1    6    0   29    0    4    3   11\n",
      "     8    0   12   18    0   25   46    0   67]\n",
      " [  30    1    0   65    1  303    0    2    0   14    0    4    3   25\n",
      "     3    1    4   20    1   31   10    0  114]\n",
      " [  13    0    0   40   12    2    2    6    0    4    0    1    1    0\n",
      "     2    1    9    4    0   11   14    0   24]\n",
      " [  39    0    0  117    1    8    0  179    2   17    0    4    1   53\n",
      "     4    3    4    4    0   52   15    0   97]\n",
      " [   4    0    0   41    1    0    0    5    7   14    0    0    0    1\n",
      "     0    0    1    1    0    9    0    0   45]\n",
      " [  24    0    0   74   20   14    1    7    2  421    6    5    3   93\n",
      "     7    5    5   15    0   35   61    0  143]\n",
      " [   9    0    0   12    1    1    0    0    0   29   50    1    0    8\n",
      "     0    2    5    8    2   15   11    0   48]\n",
      " [  26    0    0  118    2   13    0    4    0   11    0  183    9   54\n",
      "     3    1    0   31    1   12    5    0   75]\n",
      " [  21    1    0   74    1    9    0    0    0   10    0   16  119   16\n",
      "     3    9    3    9    4   10   10    0   71]\n",
      " [  11    1    0   29    4    5    0    9    0   35    2   13    3  898\n",
      "     4   10    6   50    0   16   28    1  176]\n",
      " [  11    0    0   55    6   13    0   11    0    8    0   11   10   25\n",
      "    52    5    8    5    0   36    5    0   59]\n",
      " [   8    0    0   28    5    5    0    8    0   16    2    3   28   35\n",
      "     1   25    4    9    2    3    4    0   42]\n",
      " [  12    1    0   42   12   10    0    6    1   11    3    7    6   13\n",
      "     6    3  103    7    2   47   10    0   46]\n",
      " [   3    0    0   17    7   13    0    2    0   14    5    9    4   51\n",
      "     1    4    4  209    3   10    6    0   38]\n",
      " [   1    0    0   43    3    2    0    1    0    6    1   12    1   25\n",
      "     0    0    0   52   22    8    0    0   14]\n",
      " [  19    3    0  118    4   17    0   45    0   36    1    6    8   12\n",
      "     7    1    9    7    2  351   12    0   37]\n",
      " [  17    1    0   23   31   18    0   24    0   45    5    3    3   39\n",
      "     2    0    6   16    2   37  329    0  116]\n",
      " [  12    1    0    3    2    3    0    2    0    7    1    1    2   19\n",
      "     1    0    1    3    0   21    0    1   11]\n",
      " [  78    4    0  278   47  153    3   67    5  201   24   31   36  356\n",
      "    20   13   30   48    6   89  118    1  545]]\n",
      "()\n",
      "================================================================================\n",
      "Vectorizer: Tfidf vectorizer\n",
      "Model: Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.1, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w...n', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'],\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "train time: 4.362s\n",
      "test time:  11.883s\n",
      "accuracy:   0.368\n",
      "dimensionality: 30290\n",
      "density: 1.000000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        C01       0.39      0.29      0.33       506\n",
      "        C02       0.44      0.02      0.03       233\n",
      "        C03       0.00      0.00      0.00        70\n",
      "        C04       0.44      0.75      0.55      1467\n",
      "        C05       0.50      0.09      0.15       429\n",
      "        C06       0.47      0.25      0.33       632\n",
      "        C07       0.00      0.00      0.00       146\n",
      "        C08       0.46      0.14      0.21       600\n",
      "        C09       0.00      0.00      0.00       129\n",
      "        C10       0.47      0.28      0.35       941\n",
      "        C11       0.48      0.08      0.14       202\n",
      "        C12       0.53      0.14      0.23       548\n",
      "        C13       0.48      0.12      0.19       386\n",
      "        C14       0.48      0.71      0.57      1301\n",
      "        C15       0.54      0.04      0.08       320\n",
      "        C16       0.67      0.01      0.02       228\n",
      "        C17       0.62      0.17      0.27       348\n",
      "        C18       0.50      0.34      0.40       400\n",
      "        C19       0.53      0.04      0.08       191\n",
      "        C20       0.37      0.43      0.40       695\n",
      "        C21       0.58      0.29      0.39       717\n",
      "        C22       0.00      0.00      0.00        91\n",
      "        C23       0.23      0.52      0.31      2153\n",
      "\n",
      "avg / total       0.42      0.37      0.33     12733\n",
      "\n",
      "confusion matrix:\n",
      "[[ 146    0    0   17    0   22    0   14    0    3    1   10    6   26\n",
      "     1    0    1    1    1   31    5    0  221]\n",
      " [  14    4    0   19    0   14    0    6    0    4    1    0    1    0\n",
      "     0    0    0    0    0  109    1    0   60]\n",
      " [   3    0    0    7    0    4    0    1    0    4    1    0    1    2\n",
      "     0    0    0    2    0   15    0    0   30]\n",
      " [   3    2    0 1098    0   19    0    1    0   23    1   11    4   23\n",
      "     2    0    0   10    1   34    4    0  231]\n",
      " [  16    0    0   42   38    1    0    1    0   13    0    3    0   17\n",
      "     1    0    6    6    0   16   28    0  241]\n",
      " [  14    0    0   76    0  158    0    0    0    8    0    2    1   32\n",
      "     0    0    3    9    1   17    4    0  307]\n",
      " [   6    0    0   46    2    0    0    0    0    3    0    0    0    1\n",
      "     0    0    4    1    0    5    4    0   74]\n",
      " [  24    0    0  123    0    2    0   84    0    7    0    2    0   66\n",
      "     1    0    1    1    0   48    5    0  236]\n",
      " [   4    0    0   39    0    0    0    1    0    6    0    0    0    2\n",
      "     0    0    0    0    0    6    0    0   71]\n",
      " [  17    0    0   73    3    4    0    2    0  264    1    2    1   96\n",
      "     1    0    1    5    0   23   25    0  423]\n",
      " [   7    0    0   16    0    1    0    0    0   14   16    0    0    9\n",
      "     0    0    1    4    1   11    2    0  120]\n",
      " [  13    0    0  140    1    4    0    1    0    5    0   79    2   81\n",
      "     0    0    0   18    1    9    2    0  192]\n",
      " [  13    0    0   94    0    1    0    0    0    4    0    3   46   13\n",
      "     0    0    2    2    0   11    2    0  195]\n",
      " [   8    0    0   27    1    2    0    0    0   16    0    3    1  921\n",
      "     0    0    0   16    0   10    6    0  290]\n",
      " [   8    0    0   63    2    8    0    4    0    3    0    4    3   29\n",
      "    13    0    3    1    0   27    2    0  150]\n",
      " [   2    0    0   32    0    4    0    2    0    8    1    0   12   38\n",
      "     0    2    2    1    1    2    1    0  120]\n",
      " [   4    0    0   60    3    5    0    2    0    4    0    0    0   18\n",
      "     2    0   59    5    0   40    6    0  140]\n",
      " [   1    0    0   18    1    9    0    1    0    8    1    5    0   80\n",
      "     0    0    0  134    1    5    2    0  134]\n",
      " [   0    0    0   52    1    0    0    0    0    2    0    6    1   38\n",
      "     0    0    0   28    8    4    0    0   51]\n",
      " [  15    2    0  139    2    7    0   28    0   24    0    3    2   16\n",
      "     2    0    1    4    0  298    5    0  147]\n",
      " [  10    0    0   32    8    2    0    9    0   32    1    4    2   48\n",
      "     0    0    2    3    0   26  208    0  330]\n",
      " [   7    1    0    4    0    2    0    2    0    0    1    0    1   17\n",
      "     0    0    0    0    0   17    1    0   38]\n",
      " [  42    0    0  290   14   66    0   25    0  108    8   13   11  351\n",
      "     1    1    9   15    0   40   47    0 1112]]\n",
      "()\n",
      "================================================================================\n",
      "Vectorizer: Count vectorizer\n",
      "Model: k-nearest neighbors\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.7, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w...n', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'],\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=30, p=2,\n",
      "           weights='uniform')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 8.083s\n",
      "test time:  43.163s\n",
      "accuracy:   0.254\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        C01       0.23      0.05      0.08       506\n",
      "        C02       0.21      0.01      0.02       233\n",
      "        C03       0.00      0.00      0.00        70\n",
      "        C04       0.22      0.76      0.35      1467\n",
      "        C05       0.33      0.05      0.09       429\n",
      "        C06       0.48      0.11      0.18       632\n",
      "        C07       0.00      0.00      0.00       146\n",
      "        C08       0.26      0.13      0.17       600\n",
      "        C09       0.50      0.04      0.07       129\n",
      "        C10       0.22      0.04      0.07       941\n",
      "        C11       0.42      0.05      0.09       202\n",
      "        C12       0.35      0.19      0.24       548\n",
      "        C13       0.35      0.18      0.24       386\n",
      "        C14       0.51      0.35      0.41      1301\n",
      "        C15       0.21      0.02      0.04       320\n",
      "        C16       0.00      0.00      0.00       228\n",
      "        C17       0.23      0.04      0.07       348\n",
      "        C18       0.56      0.06      0.10       400\n",
      "        C19       0.43      0.02      0.03       191\n",
      "        C20       0.43      0.15      0.22       695\n",
      "        C21       0.46      0.11      0.18       717\n",
      "        C22       0.00      0.00      0.00        91\n",
      "        C23       0.20      0.47      0.28      2153\n",
      "\n",
      "avg / total       0.31      0.25      0.21     12733\n",
      "\n",
      "confusion matrix:\n",
      "[[  23    2    0  140    4   10    0   17    0    6    0   14   10   17\n",
      "     0    0    0    1    0    7    3    0  252]\n",
      " [   6    3    0   72    0    2    0    4    1    3    0    3    6    4\n",
      "     0    0    1    0    0   46    1    0   81]\n",
      " [   1    0    0   29    0    0    0    0    0    2    1    3    2    0\n",
      "     0    0    0    0    0    1    2    0   29]\n",
      " [   2    1    0 1120    5    7    0   12    0    6    0   16   10   12\n",
      "     0    0    3    2    2    8    4    0  257]\n",
      " [   6    0    0  169   21    1    0    4    0    3    0    9    1    6\n",
      "     5    0    2    0    0    2    5    0  195]\n",
      " [   4    1    0  222    0   70    0    8    0    2    0    6    9   13\n",
      "     2    0    3    0    0    4    4    0  284]\n",
      " [   1    0    0   85    0    0    0    3    0    4    0    0    4    2\n",
      "     0    0    2    0    0    1    0    0   44]\n",
      " [   7    0    0  219    1    0    0   79    0    7    0    5    6   19\n",
      "     0    0    2    0    0    6    2    0  247]\n",
      " [   0    0    0   70    0    0    0    5    5    1    0    1    0    1\n",
      "     0    0    0    0    0    0    1    0   45]\n",
      " [   5    0    0  334    1    4    0   12    0   39    2    7    3   42\n",
      "     6    0    2    2    0   10   13    0  459]\n",
      " [   1    0    0   68    0    0    0    1    0    9   10    3    1    6\n",
      "     0    0    0    0    1    1    2    0   99]\n",
      " [   0    0    0  230    1    0    0    3    0    7    0  102    9   15\n",
      "     1    0    3    2    0    0    2    0  173]\n",
      " [   4    0    0  163    1    2    0    4    0    2    0    8   68    8\n",
      "     2    0    2    1    0    8    4    0  109]\n",
      " [   1    0    0  250    1    3    0   35    0   12    0   26    6  452\n",
      "     0    0    8    1    0    1    8    0  497]\n",
      " [   2    1    0  131    2    2    0   12    0    1    0    3    4    8\n",
      "     7    0    3    0    0    7    1    0  136]\n",
      " [   1    0    0   99    1    0    0   10    0    5    1    3    9   18\n",
      "     1    0    2    0    0    2    1    0   75]\n",
      " [   2    1    0  151    3    0    0    4    1    3    0    3    4   11\n",
      "     1    0   14    0    0    6    2    0  142]\n",
      " [   0    0    0  125    2    3    0    3    0    8    0   22    2   29\n",
      "     0    0    1   23    0    2    5    0  175]\n",
      " [   0    0    0   74    3    0    0    1    0    5    0    8    2   13\n",
      "     0    0    0    5    3    2    2    0   73]\n",
      " [   9    3    0  235    1    3    0   28    1   14    0   10   13   15\n",
      "     0    0    6    0    1  105    2    0  249]\n",
      " [   9    0    0  244    6    4    0   10    0   16    3    5    2   26\n",
      "     1    0    0    1    0   13   79    0  298]\n",
      " [   4    0    0   33    1    1    0    3    0    0    1    1    1    6\n",
      "     0    0    0    0    0    5    0    0   35]\n",
      " [  10    2    0  733   10   33    0   49    2   26    6   31   20  167\n",
      "     7    0    7    3    0    9   29    0 1009]]\n",
      "()\n",
      "================================================================================\n",
      "Vectorizer: Count vectorizer\n",
      "Model: k-nearest neighbors\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.7, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w...n', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'],\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=40, p=2,\n",
      "           weights='uniform')\n",
      "train time: 4.398s\n",
      "test time:  32.889s\n",
      "accuracy:   0.417\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        C01       0.39      0.35      0.37       506\n",
      "        C02       0.40      0.09      0.15       233\n",
      "        C03       0.67      0.26      0.37        70\n",
      "        C04       0.41      0.72      0.52      1467\n",
      "        C05       0.42      0.31      0.36       429\n",
      "        C06       0.45      0.49      0.47       632\n",
      "        C07       0.47      0.10      0.16       146\n",
      "        C08       0.41      0.27      0.32       600\n",
      "        C09       0.51      0.21      0.30       129\n",
      "        C10       0.48      0.32      0.38       941\n",
      "        C11       0.48      0.40      0.43       202\n",
      "        C12       0.47      0.42      0.44       548\n",
      "        C13       0.44      0.33      0.38       386\n",
      "        C14       0.44      0.77      0.56      1301\n",
      "        C15       0.45      0.17      0.24       320\n",
      "        C16       0.40      0.11      0.18       228\n",
      "        C17       0.51      0.28      0.36       348\n",
      "        C18       0.45      0.40      0.42       400\n",
      "        C19       0.37      0.18      0.24       191\n",
      "        C20       0.40      0.56      0.47       695\n",
      "        C21       0.56      0.43      0.48       717\n",
      "        C22       0.00      0.00      0.00        91\n",
      "        C23       0.30      0.27      0.29      2153\n",
      "\n",
      "avg / total       0.42      0.42      0.40     12733\n",
      "\n",
      "confusion matrix:\n",
      "[[ 176    3    0   29    4   26    0   23    0    7    9   29   18   36\n",
      "     3    2    2    1    1   42   17    1   77]\n",
      " [  12   21    0   25    1    8    0   19    1    5    0    1    2    6\n",
      "     0    1    0    1    0  113    1    0   16]\n",
      " [   6    0   18    9    0    4    0    1    0    4    2    0    3    4\n",
      "     1    1    0    1    0    8    0    0    8]\n",
      " [   6    4    0 1061    6   37    0   15    3   27    2   43   15   58\n",
      "     7    4    5   13   12   48    6    0   95]\n",
      " [  20    0    0   58  134    6    3    4    1   15    1    4    3   19\n",
      "     2    3   10    8    2   14   24    0   98]\n",
      " [  16    1    0   82    3  307    0    2    0    5    0    9    5   44\n",
      "     4    1    3    8    1   17   10    0  114]\n",
      " [   8    0    0   41    7    4   14    2    0    0    0    2    1    9\n",
      "     1    2    5    2    0    9    5    0   34]\n",
      " [  29    4    1  130    2   11    1  161    3    9    0   11    1   68\n",
      "     3    5    1    2    2   57   10    0   89]\n",
      " [   3    0    0   43    0    1    0    8   27    4    2    0    0    5\n",
      "     1    0    0    0    0    6    0    0   29]\n",
      " [  22    1    1  115   27   13    0    8    1  301   11    8    3  123\n",
      "     4    1    2    9    2   30   41    0  218]\n",
      " [   7    1    0   14    3    3    1    0    0   12   80    1    0   15\n",
      "     1    0    3    4    4   13    4    0   36]\n",
      " [  14    0    0  107    2   21    0    1    0    6    0  230    8   70\n",
      "     4    2    0   17    4    6    3    0   53]\n",
      " [  16    3    0   81    1    6    0    3    0    7    0   19  126   36\n",
      "     2    3    4    7    3   14   11    0   44]\n",
      " [   7    0    0   45    5    9    0   10    0   22    3   18    4 1002\n",
      "     2    4    6   26    1   15   15    0  107]\n",
      " [  10    1    2   62   12   10    2    7    0   11    0    4   11   41\n",
      "    53    0    5    3    1   31    3    0   51]\n",
      " [   5    1    0   37    5    5    1   13    0    8    3    5   29   37\n",
      "     0   26    2    2    1    4    2    0   42]\n",
      " [  10    2    0   52   10   13    1    3    1    9    3    9    3   28\n",
      "     5    1   98    5    2   32    4    0   57]\n",
      " [   5    0    0   23   10   12    0    3    0    9    8   17    6   80\n",
      "     5    4    2  159    4   10    7    0   36]\n",
      " [   2    0    0   43    4    3    0    0    0    1    1   12    2   32\n",
      "     1    0    0   43   34    2    1    0   10]\n",
      " [   6    2    0  131    5   12    0   25    1   19    1    9    2   25\n",
      "     5    1   14    8    5  392    2    0   30]\n",
      " [  13    0    1   54   26   10    0   18    2   27    7    7    4   76\n",
      "     1    0    7    6    0   35  306    0  117]\n",
      " [   9    1    1    7    1    3    0    3    0    5    1    2    4   21\n",
      "     0    0    1    1    0   15    0    0   16]\n",
      " [  45    8    3  326   49  158    7   62   13  120   34   49   34  434\n",
      "    12    4   23   28   13   65   76    0  590]]\n",
      "()\n",
      "================================================================================\n",
      "Vectorizer: Count vectorizer\n",
      "Model: Neural network\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w...n', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'],\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(102,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 775.545s\n",
      "test time:  7.153s\n",
      "accuracy:   0.425\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        C01       0.39      0.34      0.36       506\n",
      "        C02       0.44      0.13      0.20       233\n",
      "        C03       0.63      0.27      0.38        70\n",
      "        C04       0.49      0.68      0.57      1467\n",
      "        C05       0.51      0.28      0.36       429\n",
      "        C06       0.48      0.44      0.46       632\n",
      "        C07       0.60      0.06      0.11       146\n",
      "        C08       0.40      0.31      0.35       600\n",
      "        C09       0.52      0.20      0.29       129\n",
      "        C10       0.47      0.36      0.41       941\n",
      "        C11       0.48      0.36      0.41       202\n",
      "        C12       0.52      0.34      0.41       548\n",
      "        C13       0.41      0.41      0.41       386\n",
      "        C14       0.55      0.63      0.59      1301\n",
      "        C15       0.44      0.21      0.28       320\n",
      "        C16       0.43      0.11      0.18       228\n",
      "        C17       0.52      0.34      0.41       348\n",
      "        C18       0.52      0.42      0.46       400\n",
      "        C19       0.41      0.17      0.24       191\n",
      "        C20       0.43      0.47      0.44       695\n",
      "        C21       0.56      0.41      0.47       717\n",
      "        C22       0.00      0.00      0.00        91\n",
      "        C23       0.27      0.45      0.34      2153\n",
      "\n",
      "avg / total       0.44      0.43      0.42     12733\n",
      "\n",
      "confusion matrix:\n",
      "[[170   1   0  15   4  25   0  27   1  11   7  19  21  21   2   1   7   3\n",
      "    1  29  12   1 128]\n",
      " [ 15  31   0  11   0  12   0  17   1   3   0   0   9   0   0   1   1   0\n",
      "    0  94   2   0  36]\n",
      " [  5   0  19   2   0   3   0   1   0   2   2   0   2   0   1   1   1   2\n",
      "    0   5   2   0  22]\n",
      " [  3   4   0 993   1  39   0  14   3  28   3  24  25  24   8   2   7  13\n",
      "    7  43   8   0 218]\n",
      " [ 15   0   0  33 121   1   1   8   0  24   1   4   4   7   6   1   7  10\n",
      "    3  16  27   0 140]\n",
      " [ 19   3   0  52   0 281   0   2   0  13   0   3   6  22   5   1   6   4\n",
      "    2  12   5   0 196]\n",
      " [  7   1   0  40   6   1   9   3   0   5   0   1   0   2   2   1   5   0\n",
      "    0   9   1   0  53]\n",
      " [ 31   2   0 109   2   6   0 186   3   6   0   5   1  38   2   1   1   2\n",
      "    0  43  11   0 151]\n",
      " [  4   0   1  38   0   0   0   7  26   6   0   0   0   3   0   0   0   0\n",
      "    0   4   2   0  38]\n",
      " [ 17   3   0  70  11   8   0  10   2 343  11   4   3  66   8   2   2   4\n",
      "    0  23  37   0 317]\n",
      " [  7   2   1  14   1   2   1   0   0  16  73   1   0   4   0   1   4   3\n",
      "    3  13   3   0  53]\n",
      " [ 13   0   0 108   1  10   0   2   0   9   0 185  12  31   5   1   2  15\n",
      "    6   8   5   0 135]\n",
      " [ 14   3   0  58   0   3   0   1   0   8   0  16 159  16   4   4   3   4\n",
      "    3   6   7   0  77]\n",
      " [ 10   1   0  30   6   5   0  15   1  28   3  19   7 826   5   7   8  27\n",
      "    1  10  15   2 275]\n",
      " [ 10   1   2  42   7   9   0  16   0   6   0   2  14  17  66   1   5   0\n",
      "    0  19   4   0  99]\n",
      " [  5   0   0  22   4   3   0  11   0   8   1   3  32  25   1  26   6   2\n",
      "    1   2   1   0  75]\n",
      " [  6   2   0  34  11   6   0   5   1   5   3   3   7  20   6   0 117   4\n",
      "    3  24   4   0  87]\n",
      " [  2   0   0  14   6  16   0   3   0  11   7  13   7  38   1   2   2 168\n",
      "    6   5   5   0  94]\n",
      " [  1   0   0  35   3   3   0   1   0   5   2  10   6  22   0   0   0  33\n",
      "   33   3   2   0  32]\n",
      " [ 14   7   2  86   3  12   0  39   0  28   2   6  10  14   7   0  14   6\n",
      "    4 324   7   0 110]\n",
      " [ 17   1   1  25  15  14   0  26   0  33   8   4   4  37   0   0   3   5\n",
      "    0  24 297   0 203]\n",
      " [  9   2   1   4   3   4   0   4   0   5   1   1   4  14   2   0   1   1\n",
      "    0  11   1   0  23]\n",
      " [ 47   6   3 209  31 123   4  64  12 127  27  30  51 268  20   7  24  18\n",
      "    7  35  76   0 964]]\n",
      "()\n",
      "================================================================================\n",
      "Vectorizer: Tfidf vectorizer\n",
      "Model: Neural network\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w...n', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'],\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "train time: 574.902s\n",
      "test time:  5.443s\n",
      "accuracy:   0.429\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        C01       0.38      0.39      0.39       506\n",
      "        C02       0.45      0.15      0.22       233\n",
      "        C03       0.59      0.29      0.38        70\n",
      "        C04       0.48      0.69      0.57      1467\n",
      "        C05       0.49      0.29      0.37       429\n",
      "        C06       0.49      0.42      0.45       632\n",
      "        C07       0.41      0.10      0.16       146\n",
      "        C08       0.42      0.31      0.36       600\n",
      "        C09       0.48      0.19      0.27       129\n",
      "        C10       0.46      0.36      0.41       941\n",
      "        C11       0.49      0.36      0.41       202\n",
      "        C12       0.51      0.39      0.44       548\n",
      "        C13       0.41      0.36      0.39       386\n",
      "        C14       0.54      0.63      0.58      1301\n",
      "        C15       0.40      0.22      0.29       320\n",
      "        C16       0.36      0.12      0.18       228\n",
      "        C17       0.54      0.37      0.44       348\n",
      "        C18       0.51      0.45      0.48       400\n",
      "        C19       0.40      0.15      0.22       191\n",
      "        C20       0.44      0.46      0.45       695\n",
      "        C21       0.55      0.42      0.47       717\n",
      "        C22       0.50      0.01      0.02        91\n",
      "        C23       0.28      0.44      0.34      2153\n",
      "\n",
      "avg / total       0.44      0.43      0.42     12733\n",
      "\n",
      "confusion matrix:\n",
      "[[ 197    1    1   16    3   27    0   29    1    6    7   21   17   21\n",
      "     3    1    5    2    1   26   14    1  106]\n",
      " [  18   34    0   11    1   13    0   15    1    6    0    0   11    2\n",
      "     0    1    0    0    0   85    4    0   31]\n",
      " [   4    0   20    4    0    5    0    1    0    4    2    1    2    1\n",
      "     1    1    1    2    0    6    0    0   15]\n",
      " [   4    5    0 1014    3   29    2   19    2   35    4   28   21   30\n",
      "    11    3    7   10    6   30    7    0  197]\n",
      " [  17    0    0   32  126    1    3    9    0   23    1    3    4    5\n",
      "     9    0    8   13    2   15   31    0  127]\n",
      " [  23    0    1   58    1  264    0    2    0   10    0    3    7   23\n",
      "     5    1    8    4    3   11    7    0  201]\n",
      " [   7    0    0   38    5    1   14    4    0    3    0    1    1    2\n",
      "     2    1    5    1    0    8    3    0   50]\n",
      " [  34    6    1  105    2    6    1  188    6   10    0    5    1   46\n",
      "     3    1    1    2    1   45   10    0  126]\n",
      " [   4    0    1   33    0    0    0    6   24    8    0    0    0    1\n",
      "     0    0    0    1    0    4    2    0   45]\n",
      " [  19    1    0   63   16    7    1    5    2  342   12    6    3   73\n",
      "     8    4    3   10    0   21   36    0  309]\n",
      " [   8    2    1   13    2    2    1    0    0   20   72    1    0    6\n",
      "     0    2    3    2    3   13    3    0   48]\n",
      " [  18    0    0  110    1    7    0    1    0   11    0  211    9   34\n",
      "     8    1    1   18    4    7    6    0  101]\n",
      " [  21    3    1   62    1    3    0    0    0    9    0   16  139   16\n",
      "     5    7    7    4    2    5    5    0   80]\n",
      " [  11    2    0   30    5    5    0   11    0   27    1   24    4  821\n",
      "     5   10    9   26    1    8   16    0  285]\n",
      " [  10    1    0   45    6   10    1   16    0    8    0    3   11   21\n",
      "    71    1    7    0    0   21    5    0   83]\n",
      " [   7    1    0   20    4    1    1    8    0    8    1    6   26   25\n",
      "     2   28    5    3    2    1    1    0   78]\n",
      " [   6    3    0   41   10    6    1    5    1    4    3    3    5   16\n",
      "     5    1  129    3    3   23    4    0   76]\n",
      " [   2    0    0   13    7   16    0    5    0   12    6   15    5   40\n",
      "     2    3    1  180    4    5    4    0   80]\n",
      " [   1    0    0   39    4    2    0    1    0    4    0   15    5   23\n",
      "     1    0    0   37   29    3    1    0   26]\n",
      " [  17    7    2  108    3   13    1   35    1   26    1    8   11    9\n",
      "     9    0   15    6    5  320    9    0   89]\n",
      " [  15    2    1   23   17    9    0   21    1   31    9    6    3   32\n",
      "     2    1    3    6    1   26  300    0  208]\n",
      " [   9    2    3    2    3    3    0    4    0    2    1    1    4   16\n",
      "     1    0    1    1    0   13    2    1   22]\n",
      " [  62    5    2  218   37  107    8   59   11  129   27   35   46  264\n",
      "    25   10   22   25    5   37   79    0  940]]\n",
      "()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAI1CAYAAAB8GvSWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI4ZxSZIkqRHDuCRJktTI/wM0stES7cdzkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ad16630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(vectorizer, clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(vectorizer)\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    clf.fit(X_train, data_train.target)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(vectorizer.transform(data_test.data))\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(data_test.target, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(data_test.target, pred,\n",
    "                                        target_names=data_train.target_names))\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(data_test.target, pred))\n",
    "\n",
    "    print()\n",
    "    descr = str(vectorizer).split('(')[0] + \":\" + str(clf).split('(')[0]\n",
    "    return descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "for vect, vectName, clf, clfName in (\n",
    "        (CountVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1),min_df=0.0,max_df=0.7),\n",
    "         \"Count vectorizer\",\n",
    "         MultinomialNB(alpha=0.8),\n",
    "         \"Naive Bayes\"),\n",
    "        (TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1),min_df=0.0,max_df=0.1),\n",
    "         \"Tfidf vectorizer\",\n",
    "         MultinomialNB(alpha=0.1),\n",
    "         \"Naive Bayes\"),\n",
    "        (CountVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1),min_df=0.0,max_df=0.7),\n",
    "         \"Count vectorizer\",\n",
    "         KNeighborsClassifier(weights='uniform', algorithm='auto',n_neighbors=30),\n",
    "         \"k-nearest neighbors\"),\n",
    "        (TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1),min_df=0.0,max_df=0.7),\n",
    "         \"Count vectorizer\",\n",
    "         KNeighborsClassifier(weights='uniform', algorithm='auto',n_neighbors=40),\n",
    "         \"k-nearest neighbors\"),\n",
    "        (CountVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1),min_df=0.0,max_df=1.0),\n",
    "         \"Count vectorizer\",\n",
    "         MLPClassifier(hidden_layer_sizes=(int(sqrt(len(data_train.data))),)),\n",
    "         \"Neural network\"),\n",
    "         (TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range= (1, 1),min_df=0.0,max_df=1.0),\n",
    "         \"Tfidf vectorizer\",\n",
    "         MLPClassifier(hidden_layer_sizes=(100,)),\n",
    "         \"Neural network\"),\n",
    "        ):\n",
    "    print('=' * 80)\n",
    "    print(\"Vectorizer: \" + vectName)\n",
    "    print(\"Model: \" + clfName)\n",
    "    results.append(benchmark(vect, clf))\n",
    "    \n",
    "\n",
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To sum up, as we can see the accuracy of each pair as follows (decending order):<br>Vectorizer: Tfidf vectorizer, Model: Neural network, accuracy:   0.429<br>Vectorizer: Count vectorizer, Model: Neural network, accuracy:   0.425<br>Vectorizer: Count vectorizer, Model: k-nearest neighbors, accuracy:   0.417<br>Vectorizer: Count vectorizer, Model: Naive Bayes, accuracy:   0.409<br>Vectorizer: Tfidf vectorizer, Model: Naive Bayes, accuracy:   0.368<br>Vectorizer: Count vectorizer, Model: k-nearest neighbors, accuracy:   0.254<br>Meaning the best combination was tfidf vectorizer with neural network.\n",
    "### We've got to say that we found it somewhat strange that the accuracy of this test are not correleted with the best score (cross validated) of the pipeline above, and much more than that we find all of the scores we've achieved to be dissatisfying because they are pretty low :(."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
